{"cells":[{"cell_type":"markdown","metadata":{"id":"TgNxUvTlLzFG"},"source":["# Fair and Robust Sample Selection on the Synthetic Dataset\n","## With Group-Targeted Label Flipping\n","\n","#### This Jupyter Notebook simulates the proposed fair and robust sample selection on the synthetic data.\n","#### We use two fairness metrics: equalized odds and demographic parity."]},{"cell_type":"markdown","metadata":{"id":"5xHqJrLKLzFT"},"source":["## Import libraries"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_t27VU4ZMJem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643937283099,"user_tz":-540,"elapsed":226876,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}},"outputId":"d64e9bd5-99ee-450d-a7d9-db0427987239"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def weights_init_normal(m):\n","    \"\"\"Initializes the weight and bias of the model.\n","\n","    Args:\n","        m: A torch model to initialize.\n","\n","    Returns:\n","        None.\n","    \"\"\"\n","    \n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","    elif classname.find('Linear') != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.2)\n","        torch.nn.init.constant_(m.bias.data, 0)\n","\n","\n","class LogisticRegression(nn.Module):\n","    \"\"\"Logistic Regression (classifier).\n","\n","    Attributes:\n","        model: A model consisting of torch components.\n","    \"\"\"\n","    \n","    def __init__(self, n_in, n_out):\n","        \"\"\"Initializes classifier with torch components.\"\"\"\n","        \n","        super(LogisticRegression, self).__init__()\n","\n","    \n","        def block(in_feat, out_feat, normalize=True):\n","            \"\"\"Defines a block with torch components.\n","            \n","                Args:\n","                    in_feat: An integer value for the size of the input feature.\n","                    out_feat: An integer value for the size of the output feature.\n","                    normalize: A boolean indicating whether normalization is needed.\n","                    \n","                Returns:\n","                    The stacked layer.\n","            \"\"\"\n","            \n","            layers = [nn.Linear(in_feat, out_feat)]\n","            \n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(n_in,n_out)\n","        )\n","\n","    def forward(self, input_data):\n","        \"\"\"Defines a forward operation of the model.\n","        \n","        Args: \n","            input_data: The input data.\n","            \n","        Returns:\n","            The predicted label (y_hat) for the given input data.\n","        \"\"\"\n","        \n","        output = self.model(input_data)\n","        return output\n","    \n","    \n","    \n","def test_model(model_, X, y, s1):\n","    \"\"\"Tests the performance of a model.\n","\n","    Args:\n","        model_: A model to test.\n","        X: Input features of test data.\n","        y: True label (1-D) of test data.\n","        s1: Sensitive attribute (1-D) of test data.\n","\n","    Returns:\n","        The test accuracy and the fairness metrics of the model.\n","    \"\"\"\n","    \n","    model_.eval()\n","    \n","    y_hat = model_(X).squeeze()\n","    prediction = (y_hat > 0.0).int().squeeze()\n","    y = (y > 0.0).int()\n","\n","    z_0_mask = (s1 == 0.0)\n","    z_1_mask = (s1 == 1.0)\n","    z_0 = int(torch.sum(z_0_mask))\n","    z_1 = int(torch.sum(z_1_mask))\n","    \n","    y_0_mask = (y == 0.0)\n","    y_1_mask = (y == 1.0)\n","    y_0 = int(torch.sum(y_0_mask))\n","    y_1 = int(torch.sum(y_1_mask))\n","    \n","    Pr_y_hat_1 = float(torch.sum((prediction == 1))) / (z_0 + z_1)\n","    \n","    Pr_y_hat_1_z_0 = float(torch.sum((prediction == 1)[z_0_mask])) / z_0\n","    Pr_y_hat_1_z_1 = float(torch.sum((prediction == 1)[z_1_mask])) / z_1\n","        \n","    \n","    y_1_z_0_mask = (y == 1.0) & (s1 == 0.0)\n","    y_1_z_1_mask = (y == 1.0) & (s1 == 1.0)\n","    y_1_z_0 = int(torch.sum(y_1_z_0_mask))\n","    y_1_z_1 = int(torch.sum(y_1_z_1_mask))\n","    \n","    Pr_y_hat_1_y_0 = float(torch.sum((prediction == 1)[y_0_mask])) / y_0\n","    Pr_y_hat_1_y_1 = float(torch.sum((prediction == 1)[y_1_mask])) / y_1\n","    \n","    Pr_y_hat_1_y_1_z_0 = float(torch.sum((prediction == 1)[y_1_z_0_mask])) / y_1_z_0\n","    Pr_y_hat_1_y_1_z_1 = float(torch.sum((prediction == 1)[y_1_z_1_mask])) / y_1_z_1\n","    \n","    y_0_z_0_mask = (y == 0.0) & (s1 == 0.0)\n","    y_0_z_1_mask = (y == 0.0) & (s1 == 1.0)\n","    y_0_z_0 = int(torch.sum(y_0_z_0_mask))\n","    y_0_z_1 = int(torch.sum(y_0_z_1_mask))\n","\n","    Pr_y_hat_1_y_0_z_0 = float(torch.sum((prediction == 1)[y_0_z_0_mask])) / y_0_z_0\n","    Pr_y_hat_1_y_0_z_1 = float(torch.sum((prediction == 1)[y_0_z_1_mask])) / y_0_z_1\n","    \n","    recall = Pr_y_hat_1_y_1\n","    precision = float(torch.sum((prediction == 1)[y_1_mask])) / (int(torch.sum(prediction == 1)) + 0.00001)\n","    \n","    y_hat_neq_y = float(torch.sum((prediction == y.int())))\n","\n","    test_acc = torch.sum(prediction == y.int()).float() / len(y)\n","    test_f1 = 2 * recall * precision / (recall+precision+0.00001)\n","    \n","    min_dp = min(Pr_y_hat_1_z_0, Pr_y_hat_1_z_1) + 0.00001\n","    max_dp = max(Pr_y_hat_1_z_0, Pr_y_hat_1_z_1) + 0.00001\n","    min_eo_0 = min(Pr_y_hat_1_y_0_z_0, Pr_y_hat_1_y_0_z_1) + 0.00001\n","    max_eo_0 = max(Pr_y_hat_1_y_0_z_0, Pr_y_hat_1_y_0_z_1) + 0.00001\n","    min_eo_1 = min(Pr_y_hat_1_y_1_z_0, Pr_y_hat_1_y_1_z_1) + 0.00001\n","    max_eo_1 = max(Pr_y_hat_1_y_1_z_0, Pr_y_hat_1_y_1_z_1) + 0.00001\n","    \n","    DP = max(abs(Pr_y_hat_1_z_0 - Pr_y_hat_1), abs(Pr_y_hat_1_z_1 - Pr_y_hat_1))\n","    \n","    EO_Y_0 = max(abs(Pr_y_hat_1_y_0_z_0 - Pr_y_hat_1_y_0), abs(Pr_y_hat_1_y_0_z_1 - Pr_y_hat_1_y_0))\n","    EO_Y_1 = max(abs(Pr_y_hat_1_y_1_z_0 - Pr_y_hat_1_y_1), abs(Pr_y_hat_1_y_1_z_1 - Pr_y_hat_1_y_1))\n","\n","    \n","    return {'Acc': test_acc.item(), 'DP_diff': DP, 'EO_Y0_diff': EO_Y_0, 'EO_Y1_diff': EO_Y_1, 'EqOdds_diff': max(EO_Y_0, EO_Y_1)}\n","\n"],"metadata":{"id":"13kM82c6N7Xo","executionInfo":{"status":"ok","timestamp":1643937288162,"user_tz":-540,"elapsed":5071,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import sys, os\n","import numpy as np\n","import math\n","import random\n","import itertools\n","import copy\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import Sampler\n","import torch\n","\n","\n","class CustomDataset(Dataset):\n","    \"\"\"Custom Dataset.\n","\n","    Attributes:\n","        x: A PyTorch tensor for x features of data.\n","        y: A PyTorch tensor for y features (true labels) of data.\n","        z: A PyTorch tensor for z features (sensitive attributes) of data.\n","    \"\"\"\n","    def __init__(self, x_tensor, y_tensor, z_tensor):\n","        \"\"\"Initializes the dataset with torch tensors.\"\"\"\n","        \n","        self.x = x_tensor\n","        self.y = y_tensor\n","        self.z = z_tensor\n","        \n","    def __getitem__(self, index):\n","        \"\"\"Returns the selected data based on the index information.\"\"\"\n","        \n","        return (self.x[index], self.y[index], self.z[index])\n","\n","    def __len__(self):\n","        \"\"\"Returns the length of data.\"\"\"\n","        \n","        return len(self.x)\n","class CustomDataset1(Dataset):\n","    \"\"\"Custom Dataset.\n","\n","    Attributes:\n","        x: A PyTorch tensor for x features of data.\n","        y: A PyTorch tensor for y features (true labels) of data.\n","        z1: A PyTorch tensor for z1 features (sensitive attributes) of data.\n","        z2: A PyTorch tensor for z2 features (sensitive attributes) of data.\n","        z3: A PyTorch tensor for z3 features (sensitive attributes) of data.\n","    \"\"\"\n","    def __init__(self, x_tensor, y_tensor, z1_tensor, z2_tensor, z3_tensor):\n","        \"\"\"Initializes the dataset with torch tensors.\"\"\"\n","        \n","        self.x = x_tensor\n","        self.y = y_tensor\n","        self.z1 = z1_tensor\n","        self.z2 = z2_tensor\n","        self.z3 = z3_tensor\n","        \n","    def __getitem__(self, index):\n","        \"\"\"Returns the selected data based on the index information.\"\"\"\n","        \n","        return (self.x[index], self.y[index], self.z1[index], self.z2[index], self.z3[index])\n","\n","    def __len__(self):\n","        \"\"\"Returns the length of data.\"\"\"\n","        \n","        return len(self.x)    \n","class FairRobust(Sampler):\n","    \"\"\"FairRobust (Sampler in DataLoader).\n","    \n","    This class is for implementing the lambda adjustment and batch selection of FairBatch [Roh et al., ICLR 2021] with robust training.\n","\n","    Attributes:\n","        model: A model containing the intermediate states of the training.\n","        x_, y_, z_data: Tensor-based train data.\n","        alpha: A positive number for step size that used in the lambda adjustment.\n","        fairness_type: A string indicating the target fairness type \n","                       among original, demographic parity (dp), equal opportunity (eqopp), and equalized odds (eqodds).\n","        replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        N: An integer counting the size of data.\n","        batch_size: An integer for the size of a batch.\n","        batch_num: An integer for total number of batches in an epoch.\n","        y_, z_item: Lists that contains the unique values of the y_data and z_data, respectively.\n","        yz_tuple: Lists for pairs of y_item and z_item.\n","        y_, z_, yz_mask: Dictionaries utilizing as array masks.\n","        y_, z_, yz_index: Dictionaries containing the indexes of each class.\n","        y_, z_, yz_len: Dictionaries containing the length information.\n","        clean_index: A list that contains the data indexes of selected samples.\n","        clean_y_, clean_z_, clean_yz_index: Dictionaries containing the indexes of each class in the selected set.\n","        clean_y_, clean_z_, clean_yz_len: Dictionaries containing the length information in the selected set.\n","        S: A dictionary containing the default size of each class in a batch.\n","        lb1, lb2: (0~1) real numbers indicating the lambda values for fairness [Roh et al., ICLR 2021].\n","        tau: (0~1) real number indicating the clean ratio of the data.\n","        warm_start: An integer for warm-start period.\n","\n","        \n","    \"\"\"\n","    def __init__(self, model, x_tensor, y_tensor, z_tensor, target_fairness, parameters, replacement = False, seed = 0):\n","        \"\"\"Initializes FairBatch.\"\"\"\n","        \n","        self.model = model\n","        \n","        np.random.seed(seed)\n","        random.seed(seed)\n","        \n","        self.x_data = x_tensor\n","        self.y_data = y_tensor\n","        self.z_data = z_tensor\n","        \n","        \n","        self.alpha = parameters.alpha\n","        self.fairness_type = target_fairness\n","        \n","        self.replacement = replacement\n","        \n","        self.N = len(z_tensor)\n","        \n","        self.batch_size = parameters.batch_size\n","        self.batch_num = int(len(self.y_data) / self.batch_size)\n","        \n","        # Takes the unique values of the tensors\n","        self.z_item = list(set(z_tensor.tolist()))\n","        self.y_item = list(set(y_tensor.tolist()))\n","        \n","        self.yz_tuple = list(itertools.product(self.y_item, self.z_item))\n","        \n","        # Makes masks\n","        self.z_mask = {}\n","        self.y_mask = {}\n","        self.yz_mask = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.z_mask[tmp_z] = (self.z_data == tmp_z)\n","            \n","        for tmp_y in self.y_item:\n","            self.y_mask[tmp_y] = (self.y_data == tmp_y)\n","            \n","        for tmp_yz in self.yz_tuple:\n","            self.yz_mask[tmp_yz] = (self.y_data == tmp_yz[0]) & (self.z_data == tmp_yz[1])\n","        \n","\n","        # Finds the index\n","        self.z_index = {}\n","        self.y_index = {}\n","        self.yz_index = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.z_index[tmp_z] = (self.z_mask[tmp_z] == 1).nonzero().squeeze()\n","            \n","        for tmp_y in self.y_item:\n","            self.y_index[tmp_y] = (self.y_mask[tmp_y] == 1).nonzero().squeeze()\n","        \n","        for tmp_yz in self.yz_tuple:\n","            self.yz_index[tmp_yz] = (self.yz_mask[tmp_yz] == 1).nonzero().squeeze()\n","            \n","        self.entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","            \n","        # Length information\n","        self.z_len = {}\n","        self.y_len = {}\n","        self.yz_len = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.z_len[tmp_z] = len(self.z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            self.y_len[tmp_y] = len(self.y_index[tmp_y])\n","            \n","        for tmp_yz in self.yz_tuple:\n","            self.yz_len[tmp_yz] = len(self.yz_index[tmp_yz])\n","\n","        # Default batch size\n","        self.S = {}\n","        \n","        for tmp_yz in self.yz_tuple:\n","            self.S[tmp_yz] = self.batch_size * (self.yz_len[tmp_yz])/self.N\n","\n","        \n","        self.lb1 = (self.S[1,1])/(self.S[1,1]+(self.S[1,0]))\n","        self.lb2 = (self.S[-1,1])/(self.S[-1,1]+(self.S[-1,0]))\n","        \n","        # For cleanselection parameters\n","        self.tau = parameters.tau # Clean ratio\n","        self.warm_start = parameters.warm_start\n","    \n","        self.count_epoch = 0\n","        \n","            \n","        # Clean sample selection\n","        self.clean_index = np.arange(0,len(self.y_data))\n","        \n","        # Finds the index\n","        self.clean_z_index = {}\n","        self.clean_y_index = {}\n","        self.clean_yz_index = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.clean_z_index[tmp_z] = (self.z_mask[tmp_z] == 1)[self.clean_index].nonzero().squeeze()\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_index[tmp_y] = (self.y_mask[tmp_y] == 1)[self.clean_index].nonzero().squeeze()\n","        \n","        for tmp_yz in self.yz_tuple:\n","            self.clean_yz_index[tmp_yz] = (self.yz_mask[tmp_yz] == 1)[self.clean_index].nonzero().squeeze()\n","        \n","        \n","       # Length information\n","        self.clean_z_len = {}\n","        self.clean_y_len = {}\n","        self.clean_yz_len = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.clean_z_len[tmp_z] = len(self.clean_z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","            \n","        for tmp_yz in self.yz_tuple:\n","            self.clean_yz_len[tmp_yz] = len(self.clean_yz_index[tmp_yz])\n"," \n","      \n","    def get_logit(self):\n","        \"\"\"Runs forward pass of the intermediate model with the training data.\n","        \n","        Returns:\n","            Outputs (logits) of the model.\n","\n","        \"\"\"\n","        \n","        self.model.eval()\n","        logit = self.model(self.x_data)\n","        \n","        return logit\n","    \n","    \n","    def adjust_lambda(self, logit):\n","        \"\"\"Adjusts the lambda values using FairBatch [Roh et al., ICLR 2021].\n","        See our paper for algorithm details.\n","        \n","        Args: \n","            logit: A torch tensor that contains the intermediate model's output on the training data.\n","        \n","        \"\"\"\n","        \n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        \n","        if self.fairness_type == 'eqopp':\n","            \n","            yhat_yz = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz in self.yz_tuple:\n","                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.clean_yz_index[tmp_yz]])) / self.clean_yz_len[tmp_yz]\n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / self.clean_y_len[tmp_y]\n","            \n","            # lb1 * loss_z1 + (1-lb1) * loss_z0\n","            \n","            if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","                self.lb1 += self.alpha\n","            else:\n","                self.lb1 -= self.alpha\n","                \n","            if self.lb1 < 0:\n","                self.lb1 = 0\n","            elif self.lb1 > 1:\n","                self.lb1 = 1 \n","                \n","        elif self.fairness_type == 'eqodds':\n","            \n","            yhat_yz = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz in self.yz_tuple:\n","                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.clean_yz_index[tmp_yz]])) / (self.clean_yz_len[tmp_yz]+1)\n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / (self.clean_y_len[tmp_y]+1)\n","            \n","            y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n","            y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff > y0_diff:\n","                if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","                    self.lb1 += self.alpha\n","                else:\n","                    self.lb1 -= self.alpha\n","            else:\n","                if yhat_yz[(-1, 1)] > yhat_yz[(-1, 0)]:\n","                    self.lb2 += self.alpha\n","                else:\n","                    self.lb2 -= self.alpha\n","                    \n","                \n","            if self.lb1 < 0:\n","                self.lb1 = 0\n","            elif self.lb1 > 1:\n","                self.lb1 = 1\n","                \n","            if self.lb2 < 0:\n","                self.lb2 = 0\n","            elif self.lb2 > 1:\n","                self.lb2 = 1\n","                \n","        elif self.fairness_type == 'dp':\n","            yhat_yz = {}\n","            yhat_y = {}\n","            \n","            ones_array = np.ones(len(self.y_data))\n","            ones_tensor = torch.FloatTensor(ones_array).cuda()\n","            dp_loss = criterion((F.tanh(logit.squeeze())+1)/2, ones_tensor.squeeze()) # Note that ones tensor puts as the true label\n","            \n","            for tmp_yz in self.yz_tuple:\n","                yhat_yz[tmp_yz] = float(torch.sum(dp_loss[self.clean_yz_index[tmp_yz]])) / self.clean_z_len[tmp_yz[1]]\n","                    \n","            \n","            y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n","            y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff > y0_diff:\n","                if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","                    self.lb1 += self.alpha\n","                else:\n","                    self.lb1 -= self.alpha\n","            else:\n","                if yhat_yz[(-1, 1)] > yhat_yz[(-1, 0)]: \n","                    self.lb2 -= self.alpha\n","                else:\n","                    self.lb2 += self.alpha\n","                    \n","            if self.lb1 < 0:\n","                self.lb1 = 0\n","            elif self.lb1 > 1:\n","                self.lb1 = 1\n","                \n","            if self.lb2 < 0:\n","                self.lb2 = 0\n","            elif self.lb2 > 1:\n","                self.lb2 = 1\n","\n","    def select_fair_robust_sample(self):\n","        \"\"\"Selects fair and robust samples and adjusts the lambda values for fairness. \n","        See our paper for algorithm details.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        logit = self.get_logit()\n","        \n","        self.adjust_lambda(logit)\n","\n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","        profit = torch.max(loss)-loss\n","        \n","        current_weight_sum = {}\n","        \n","        lb_ratio = {}\n","        \n","        for tmp_yz in self.yz_tuple:\n","            if tmp_yz == (1,1):\n","                lb_ratio[tmp_yz] = self.lb1\n","            elif tmp_yz == (1,0): \n","                lb_ratio[tmp_yz] = 1-self.lb1\n","            elif tmp_yz == (-1,1):\n","                lb_ratio[tmp_yz] = self.lb2\n","            elif tmp_yz == (-1,0):\n","                lb_ratio[tmp_yz] = 1-self.lb2\n","            \n","            current_weight_sum[tmp_yz] = 0\n","        \n","        # Greedy-based algorithm\n","        \n","        (_, sorted_index) = torch.topk(profit, len(profit), largest=True, sorted=True)\n","        \n","        clean_index = []\n","        \n","        total_selected = 0\n","        \n","        desired_size = int(self.tau * len(self.y_data))\n","        \n","        for j in sorted_index:\n","            tmp_y = self.y_data[j].item()\n","            tmp_z = self.z_data[j].item()\n","            current_weight_list = list(current_weight_sum.values())\n","            \n","            if total_selected >= desired_size:\n","                break\n","            if all(i < desired_size for i in current_weight_list):\n","                clean_index.append(j)\n","                \n","                current_weight_sum[(tmp_y, tmp_z)] += 2 - lb_ratio[(tmp_y, tmp_z)]\n","                current_weight_sum[(tmp_y, 1-tmp_z)] += 1 - lb_ratio[(tmp_y, 1-tmp_z)]\n","                current_weight_sum[(tmp_y * -1, tmp_z)] += 1\n","                current_weight_sum[(tmp_y * -1, 1-tmp_z)] += 1\n","                \n","                total_selected += 1        \n","        \n","        clean_index = torch.LongTensor(clean_index).cuda()\n","        \n","        self.batch_num = int(len(clean_index)/self.batch_size)\n","        \n","        # Update the variables\n","        self.clean_index = clean_index\n","        \n","        for tmp_z in self.z_item:\n","            combined = torch.cat((self.z_index[tmp_z], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_z_index[tmp_z] = intersection \n","            \n","        for tmp_y in self.y_item:\n","            combined = torch.cat((self.y_index[tmp_y], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_y_index[tmp_y] = intersection\n","        \n","        for tmp_yz in self.yz_tuple:\n","            combined = torch.cat((self.yz_index[tmp_yz], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_yz_index[tmp_yz] = intersection\n","        \n","        \n","        for tmp_z in self.z_item:\n","            self.clean_z_len[tmp_z] = len(self.clean_z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","        \n","        for tmp_yz in self.yz_tuple:\n","            self.clean_yz_len[tmp_yz] = len(self.clean_yz_index[tmp_yz])\n","            \n","        \n","        return clean_index\n","        \n","    \n","    def select_batch_replacement(self, batch_size, full_index, batch_num, replacement = False, weight = None):\n","        \"\"\"Selects a certain number of batches based on the given batch size.\n","        \n","        Args: \n","            batch_size: An integer for the data size in a batch.\n","            full_index: An array containing the candidate data indices.\n","            batch_num: An integer indicating the number of batches.\n","            replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        select_index = []\n","        \n","        if replacement == True:\n","            for _ in range(batch_num):\n","                if weight == None:\n","                    weight_norm = weight/torch.sum(weight)\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False, p = weight_norm))\n","                else:\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False))\n","        else:\n","            tmp_index = full_index.detach().cpu().numpy().copy()\n","            random.shuffle(tmp_index)\n","            \n","            start_idx = 0\n","            for i in range(batch_num):\n","                if start_idx + batch_size > len(full_index):\n","                    select_index.append(np.concatenate((tmp_index[start_idx:], tmp_index[ : batch_size - (len(full_index)-start_idx)])))\n","                    \n","                    start_idx = len(full_index)-start_idx\n","                else:\n","\n","                    select_index.append(tmp_index[start_idx:start_idx + batch_size])\n","                    start_idx += batch_size\n","            \n","        return select_index\n","\n","\n","    \n","    def decide_fair_batch_size(self):\n","        \"\"\"Calculates each class size based on the lambda values (lb1 and lb2) for fairness.\n","        \n","        Returns:\n","            Each class size for fairness.\n","            \n","        \"\"\"\n","        \n","        each_size = {}\n","\n","        for tmp_yz in self.yz_tuple:\n","            self.S[tmp_yz] = self.batch_size * (self.clean_yz_len[tmp_yz])/len(self.clean_index)\n","\n","        # Based on the updated lambdas, determine the size of each class in a batch\n","        if self.fairness_type == 'eqopp':\n","            # lb1 * loss_z1 + (1-lb1) * loss_z0\n","\n","            each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(-1,1)] = round(self.S[(-1,1)])\n","            each_size[(-1,0)] = round(self.S[(-1,0)])\n","\n","        elif self.fairness_type == 'eqodds':\n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","\n","            each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(-1,1)] = round(self.lb2 * (self.S[(-1,1)] + self.S[(-1,0)]))\n","            each_size[(-1,0)] = round((1-self.lb2) * (self.S[(-1,1)] + self.S[(-1,0)]))\n","\n","        elif self.fairness_type == 'dp':\n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","\n","            each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n","            each_size[(-1,1)] = round(self.lb2 * (self.S[(-1,1)] + self.S[(-1,0)]))\n","            each_size[(-1,0)] = round((1-self.lb2) * (self.S[(-1,1)] + self.S[(-1,0)]))\n","        \n","        return each_size\n","        \n","        \n","    \n","    def __iter__(self):\n","        \"\"\"Iters the full process of fair and robust sample selection for serving the batches to training.\n","        \n","        Returns:\n","            Indexes that indicate the data in each batch.\n","            \n","        \"\"\"\n","        self.count_epoch += 1\n","        \n","        if self.count_epoch > self.warm_start:\n","\n","            _ = self.select_fair_robust_sample()\n","\n","\n","            each_size = self.decide_fair_batch_size()\n","\n","            # Get the indices for each class\n","            sort_index_y_1_z_1 = self.select_batch_replacement(each_size[(1, 1)], self.clean_yz_index[(1,1)], self.batch_num, self.replacement)\n","            sort_index_y_0_z_1 = self.select_batch_replacement(each_size[(-1, 1)], self.clean_yz_index[(-1,1)], self.batch_num, self.replacement)\n","            sort_index_y_1_z_0 = self.select_batch_replacement(each_size[(1, 0)], self.clean_yz_index[(1,0)], self.batch_num, self.replacement)\n","            sort_index_y_0_z_0 = self.select_batch_replacement(each_size[(-1, 0)], self.clean_yz_index[(-1,0)], self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                key_in_fairbatch = sort_index_y_0_z_0[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_0[i].copy()))\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_0_z_1[i].copy()))\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_1[i].copy()))\n","\n","                random.shuffle(key_in_fairbatch)\n","\n","                yield key_in_fairbatch\n","\n","        else:\n","            entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","\n","            sort_index = self.select_batch_replacement(self.batch_size, entire_index, self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                yield sort_index[i]\n","        \n","                               \n","\n","    def __len__(self):\n","        \"\"\"Returns the length of data.\"\"\"\n","        \n","        return len(self.y_data)\n","class FairRobust1(Sampler):\n","    \"\"\"FairRobust (Sampler in DataLoader).\n","    \n","    This class is for implementing the lambda adjustment and batch selection of FairBatch [Roh et al., ICLR 2021] with robust training.\n","\n","    Attributes:\n","        model: A model containing the intermediate states of the training.\n","        x_, y_, z_data: Tensor-based train data.\n","        alpha: A positive number for step size that used in the lambda adjustment.\n","        fairness_type: A string indicating the target fairness type \n","                       among original, demographic parity (dp), equal opportunity (eqopp), and equalized odds (eqodds).\n","        replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        N: An integer counting the size of data.\n","        batch_size: An integer for the size of a batch.\n","        batch_num: An integer for total number of batches in an epoch.\n","        y_, z_item: Lists that contains the unique values of the y_data and z_data, respectively.\n","        yz_tuple: Lists for pairs of y_item and z_item.\n","        y_, z_, yz_mask: Dictionaries utilizing as array masks.\n","        y_, z_, yz_index: Dictionaries containing the indexes of each class.\n","        y_, z_, yz_len: Dictionaries containing the length information.\n","        clean_index: A list that contains the data indexes of selected samples.\n","        clean_y_, clean_z_, clean_yz_index: Dictionaries containing the indexes of each class in the selected set.\n","        clean_y_, clean_z_, clean_yz_len: Dictionaries containing the length information in the selected set.\n","        S: A dictionary containing the default size of each class in a batch.\n","        lb1, lb2: (0~1) real numbers indicating the lambda values for fairness [Roh et al., ICLR 2021].\n","        tau: (0~1) real number indicating the clean ratio of the data.\n","        warm_start: An integer for warm-start period.\n","\n","        \n","    \"\"\"\n","    def __init__(self, model, x_tensor, y_tensor, z1_tensor, z2_tensor, z3_tensor, target_fairness, parameters, replacement = False, seed = 0):\n","        \"\"\"Initializes FairBatch.\"\"\"\n","        \n","        self.model = model\n","        \n","        np.random.seed(seed)\n","        random.seed(seed)\n","        \n","        self.x_data = x_tensor\n","        self.y_data = y_tensor\n","        self.z1_data = z1_tensor\n","        self.z2_data = z2_tensor\n","        self.z3_data = z3_tensor\n","        \n","        #alpha: how to modify rate of each category\n","        self.alpha = parameters.alpha\n","        self.fairness_type = target_fairness\n","        \n","        self.replacement = replacement\n","        # N: number of elements\n","        self.N = len(z1_tensor)\n","        \n","        self.batch_size = parameters.batch_size\n","        self.batch_num = int(len(self.y_data) / self.batch_size)\n","        \n","        # Takes the unique values of the tensors\n","        self.z1_item = list(set(z1_tensor.tolist()))\n","        self.z2_item = list(set(z2_tensor.tolist()))\n","        self.z3_item = list(set(z3_tensor.tolist()))\n","        self.y_item = list(set(y_tensor.tolist()))\n","        # yz_tuple: (-1,0), (-1,1), (1,1), (1,0)\n","        self.yz1_tuple = list(itertools.product(self.y_item, self.z1_item))\n","        self.yz2_tuple = list(itertools.product(self.y_item, self.z2_item))\n","        self.yz3_tuple = list(itertools.product(self.y_item, self.z3_item))\n","        \n","        # Makes masks\n","        #z_mask: z = 1 or z = 0 (list)\n","        self.z1_mask = {}\n","        self.z2_mask = {}\n","        self.z3_mask = {}\n","        #z_mask: y = 1 or y = -1 (list)\n","        self.y_mask = {}\n","        self.yz1_mask = {}\n","        self.yz2_mask = {}\n","        self.yz3_mask = {}\n","        \n","        for tmp_z1 in self.z1_item:\n","            self.z1_mask[tmp_z1] = (self.z1_data == tmp_z1)\n","        \n","        for tmp_z2 in self.z2_item:\n","            self.z2_mask[tmp_z2] = (self.z2_data == tmp_z2)\n","        \n","        for tmp_z3 in self.z3_item:\n","            self.z3_mask[tmp_z3] = (self.z3_data == tmp_z3)\n","            \n","        for tmp_y in self.y_item:\n","            self.y_mask[tmp_y] = (self.y_data == tmp_y)\n","            \n","        for tmp_yz1 in self.yz1_tuple:\n","            self.yz1_mask[tmp_yz1] = (self.y_data == tmp_yz1[0]) & (self.z1_data == tmp_yz1[1])\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.yz2_mask[tmp_yz2] = (self.y_data == tmp_yz2[0]) & (self.z2_data == tmp_yz2[1])\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.yz3_mask[tmp_yz3] = (self.y_data == tmp_yz3[0]) & (self.z3_data == tmp_yz3[1])\n","        \n","\n","        # Finds the index\n","        self.z1_index = {}\n","        self.z2_index = {}\n","        self.z3_index = {}\n","        self.y_index = {}\n","        self.yz1_index = {}\n","        self.yz2_index = {}\n","        self.yz3_index = {}\n","        #index of z=1, z=0\n","        for tmp_z1 in self.z1_item:\n","            self.z1_index[tmp_z1] = (self.z1_mask[tmp_z1] == 1).nonzero().squeeze()\n","        for tmp_z2 in self.z2_item:\n","            self.z2_index[tmp_z2] = (self.z2_mask[tmp_z2] == 1).nonzero().squeeze()\n","        for tmp_z3 in self.z3_item:\n","            self.z3_index[tmp_z3] = (self.z3_mask[tmp_z3] == 1).nonzero().squeeze()\n","        #index of y=-1, y=0\n","        for tmp_y in self.y_item:\n","            self.y_index[tmp_y] = (self.y_mask[tmp_y] == 1).nonzero().squeeze()\n","        #index of (1,1); (-1,1); (1,0); (-1,0)\n","        for tmp_yz1 in self.yz1_tuple:\n","            self.yz1_index[tmp_yz1] = (self.yz1_mask[tmp_yz1] == 1).nonzero().squeeze()\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.yz2_index[tmp_yz2] = (self.yz2_mask[tmp_yz2] == 1).nonzero().squeeze()\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.yz3_index[tmp_yz3] = (self.yz3_mask[tmp_yz3] == 1).nonzero().squeeze()\n","            \n","        self.entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","            \n","        # Length information\n","        self.z1_len = {}\n","        self.z2_len = {}\n","        self.z3_len = {}\n","        self.y_len = {}\n","        self.yz1_len = {}\n","        self.yz2_len = {}\n","        self.yz3_len = {}\n","        \n","        for tmp_z1 in self.z1_item:\n","            self.z1_len[tmp_z1] = len(self.z1_index[tmp_z1])\n","        for tmp_z2 in self.z2_item:\n","            self.z2_len[tmp_z2] = len(self.z2_index[tmp_z2])\n","        for tmp_z3 in self.z3_item:\n","            self.z3_len[tmp_z3] = len(self.z3_index[tmp_z3])\n","            \n","        for tmp_y in self.y_item:\n","            self.y_len[tmp_y] = len(self.y_index[tmp_y])\n","            \n","        for tmp_yz1 in self.yz1_tuple:\n","            self.yz1_len[tmp_yz1] = len(self.yz1_index[tmp_yz1])\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.yz2_len[tmp_yz2] = len(self.yz2_index[tmp_yz2])\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.yz3_len[tmp_yz3] = len(self.yz3_index[tmp_yz3])\n","\n","        # Default batch size\n","        self.S1 = {}\n","        self.S2 = {}\n","        self.S3 = {}\n","        \n","        # len of each (y,z)\n","        for tmp_yz1 in self.yz1_tuple:\n","            self.S1[tmp_yz1] = self.batch_size * (self.yz1_len[tmp_yz1])/self.N\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.S2[tmp_yz2] = self.batch_size * (self.yz2_len[tmp_yz2])/self.N\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.S3[tmp_yz3] = self.batch_size * (self.yz3_len[tmp_yz3])/self.N\n","\n","        # lambda = (y=1,z=1)/(y=1)\n","        self.z1_lb1 = (self.S1[1,1])/(self.S1[1,1]+(self.S1[1,0]))\n","        # (y=-1,z=1)/(y=-1)\n","        self.z1_lb2 = (self.S1[-1,1])/(self.S1[-1,1]+(self.S1[-1,0]))\n","        \n","        # lambda = (y=1,z=1)/(y=1)\n","        self.z2_lb1 = (self.S2[1,1])/(self.S2[1,1]+(self.S2[1,0]))\n","        # (y=-1,z=1)/(y=-1)\n","        self.z2_lb2 = (self.S2[-1,1])/(self.S2[-1,1]+(self.S2[-1,0]))\n","        \n","        # lambda = (y=1,z=1)/(y=1)\n","        self.z3_lb1 = (self.S3[1,1])/(self.S3[1,1]+(self.S3[1,0]))\n","        # (y=-1,z=1)/(y=-1)\n","        self.z3_lb2 = (self.S3[-1,1])/(self.S3[-1,1]+(self.S3[-1,0]))\n","        \n","        # For cleanselection parameters\n","        self.tau = parameters.tau # Clean ratio\n","        self.warm_start = parameters.warm_start\n","    \n","        self.count_epoch = 0\n","        \n","            \n","        # Clean sample selection\n","        self.clean_index = np.arange(0,len(self.y_data))\n","        \n","        # Finds the index\n","        self.clean_z1_index = {}\n","        self.clean_z2_index = {}\n","        self.clean_z3_index = {}\n","        self.clean_y_index = {}\n","        self.clean_yz1_index = {}\n","        self.clean_yz2_index = {}\n","        self.clean_yz3_index = {}\n","        # index of each z values in list\n","        for tmp_z1 in self.z1_item:\n","            self.clean_z1_index[tmp_z1] = (self.z1_mask[tmp_z1] == 1)[self.clean_index].nonzero().squeeze()\n","        for tmp_z2 in self.z2_item:\n","            self.clean_z2_index[tmp_z2] = (self.z2_mask[tmp_z2] == 1)[self.clean_index].nonzero().squeeze()\n","        for tmp_z3 in self.z3_item:\n","            self.clean_z3_index[tmp_z3] = (self.z3_mask[tmp_z3] == 1)[self.clean_index].nonzero().squeeze()\n","        # index of each y values in list\n","        for tmp_y in self.y_item:\n","            self.clean_y_index[tmp_y] = (self.y_mask[tmp_y] == 1)[self.clean_index].nonzero().squeeze()\n","        # index of each yz values in list\n","        for tmp_yz1 in self.yz1_tuple:\n","            self.clean_yz1_index[tmp_yz1] = (self.yz1_mask[tmp_yz1] == 1)[self.clean_index].nonzero().squeeze()\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.clean_yz2_index[tmp_yz2] = (self.yz2_mask[tmp_yz2] == 1)[self.clean_index].nonzero().squeeze()\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.clean_yz3_index[tmp_yz3] = (self.yz3_mask[tmp_yz3] == 1)[self.clean_index].nonzero().squeeze()\n","        \n","        \n","       # Length information\n","        self.clean_z1_len = {}\n","        self.clean_z2_len = {}\n","        self.clean_z3_len = {}\n","        self.clean_y_len = {}\n","        self.clean_yz1_len = {}\n","        self.clean_yz2_len = {}\n","        self.clean_yz3_len = {}\n","        \n","        for tmp_z1 in self.z1_item:\n","            self.clean_z1_len[tmp_z1] = len(self.clean_z1_index[tmp_z1])\n","        for tmp_z2 in self.z2_item:\n","            self.clean_z2_len[tmp_z2] = len(self.clean_z2_index[tmp_z2])\n","        for tmp_z3 in self.z3_item:\n","            self.clean_z3_len[tmp_z3] = len(self.clean_z3_index[tmp_z3])\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","            \n","        for tmp_yz1 in self.yz1_tuple:\n","            self.clean_yz1_len[tmp_yz1] = len(self.clean_yz1_index[tmp_yz1])\n","        for tmp_yz2 in self.yz2_tuple:\n","            self.clean_yz2_len[tmp_yz2] = len(self.clean_yz2_index[tmp_yz2])\n","        for tmp_yz3 in self.yz3_tuple:\n","            self.clean_yz3_len[tmp_yz3] = len(self.clean_yz3_index[tmp_yz3])\n"," \n","      \n","    def get_logit(self):\n","        \"\"\"Runs forward pass of the intermediate model with the training data.\n","        \n","        Returns:\n","            Outputs (logits) of the model.\n","\n","        \"\"\"\n","        \n","        self.model.eval()\n","        logit = self.model(self.x_data)\n","        \n","        return logit\n","    \n","    \n","    def adjust_lambda(self, logit):\n","        \"\"\"Adjusts the lambda values using FairBatch [Roh et al., ICLR 2021].\n","        See our paper for algorithm details.\n","        \n","        Args: \n","            logit: A torch tensor that contains the intermediate model's output on the training data.\n","        \n","        \"\"\"\n","        \n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        \n","        if self.fairness_type == 'eqopp':\n","            \n","            yhat_yz1 = {}\n","            yhat_yz2 = {}\n","            yhat_yz3 = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz1 in self.yz1_tuple:\n","                yhat_yz1[tmp_yz1] = float(torch.sum(eo_loss[self.clean_yz1_index[tmp_yz1]])) / self.clean_yz1_len[tmp_yz1]\n","            for tmp_yz2 in self.yz2_tuple:\n","                yhat_yz2[tmp_yz2] = float(torch.sum(eo_loss[self.clean_yz2_index[tmp_yz2]])) / self.clean_yz2_len[tmp_yz2]\n","            for tmp_yz3 in self.yz3_tuple:\n","                yhat_yz3[tmp_yz3] = float(torch.sum(eo_loss[self.clean_yz3_index[tmp_yz3]])) / self.clean_yz3_len[tmp_yz3]\n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / self.clean_y_len[tmp_y]\n","            \n","            # lb1 * loss_z1 + (1-lb1) * loss_z0\n","            if yhat_yz1[(1, 1)] > yhat_yz1[(1, 0)]:\n","                self.z1_lb1 += self.alpha\n","            else:\n","                self.z1_lb1 -= self.alpha\n","                \n","            if yhat_yz2[(1, 1)] > yhat_yz2[(1, 0)]:\n","                self.z2_lb1 += self.alpha\n","            else:\n","                self.z2_lb1 -= self.alpha\n","                \n","            if yhat_yz3[(1, 1)] > yhat_yz3[(1, 0)]:\n","                self.z3_lb1 += self.alpha\n","            else:\n","                self.z3_lb1 -= self.alpha\n","                \n","            if self.z1_lb1 < 0:\n","                self.z1_lb1 = 0\n","            elif self.z1_lb1 > 1:\n","                self.z1_lb1 = 1 \n","                \n","            if self.z2_lb1 < 0:\n","                self.z2_lb1 = 0\n","            elif self.z2_lb1 > 1:\n","                self.z2_lb1 = 1 \n","                \n","            if self.z3_lb1 < 0:\n","                self.z3_lb1 = 0\n","            elif self.z3_lb1 > 1:\n","                self.z3_lb1 = 1     \n","        elif self.fairness_type == 'eqodds':\n","            \n","            yhat_yz1 = {}\n","            yhat_yz2 = {}\n","            yhat_yz3 = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz1 in self.yz1_tuple:\n","                yhat_yz1[tmp_yz1] = float(torch.sum(eo_loss[self.clean_yz1_index[tmp_yz1]])) / (self.clean_yz1_len[tmp_yz1]+1)\n","            \n","            for tmp_yz2 in self.yz2_tuple:\n","                yhat_yz2[tmp_yz2] = float(torch.sum(eo_loss[self.clean_yz2_index[tmp_yz2]])) / (self.clean_yz2_len[tmp_yz2]+1)\n","                \n","            for tmp_yz3 in self.yz3_tuple:\n","                yhat_yz3[tmp_yz3] = float(torch.sum(eo_loss[self.clean_yz3_index[tmp_yz3]])) / (self.clean_yz3_len[tmp_yz3]+1)\n","            \n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / (self.clean_y_len[tmp_y]+1)\n","            \n","            y1_diff_z1 = abs(yhat_yz1[(1, 1)] - yhat_yz1[(1, 0)])\n","            y0_diff_z1 = abs(yhat_yz1[(-1, 1)] - yhat_yz1[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z1 > y0_diff_z1:\n","                if yhat_yz1[(1, 1)] > yhat_yz1[(1, 0)]:\n","                    self.z1_lb1 += self.alpha\n","                else:\n","                    self.z1_lb1 -= self.alpha\n","            else:\n","                if yhat_yz1[(-1, 1)] > yhat_yz1[(-1, 0)]:\n","                    self.z1_lb2 += self.alpha\n","                else:\n","                    self.z1_lb2 -= self.alpha\n","            \n","            y1_diff_z2 = abs(yhat_yz2[(1, 1)] - yhat_yz2[(1, 0)])\n","            y0_diff_z2 = abs(yhat_yz2[(-1, 1)] - yhat_yz2[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z2 > y0_diff_z2:\n","                if yhat_yz2[(1, 1)] > yhat_yz2[(1, 0)]:\n","                    self.z2_lb1 += self.alpha\n","                else:\n","                    self.z2_lb1 -= self.alpha\n","            else:\n","                if yhat_yz2[(-1, 1)] > yhat_yz2[(-1, 0)]:\n","                    self.z2_lb2 += self.alpha\n","                else:\n","                    self.z2_lb2 -= self.alpha\n","                    \n","            y1_diff_z3 = abs(yhat_yz3[(1, 1)] - yhat_yz3[(1, 0)])\n","            y0_diff_z3 = abs(yhat_yz3[(-1, 1)] - yhat_yz3[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z3 > y0_diff_z3:\n","                if yhat_yz3[(1, 1)] > yhat_yz3[(1, 0)]:\n","                    self.z3_lb1 += self.alpha\n","                else:\n","                    self.z3_lb1 -= self.alpha\n","            else:\n","                if yhat_yz3[(-1, 1)] > yhat_yz3[(-1, 0)]:\n","                    self.z3_lb2 += self.alpha\n","                else:\n","                    self.z3_lb2 -= self.alpha\n","                    \n","                \n","            if self.z1_lb1 < 0:\n","                self.z1_lb1 = 0\n","            elif self.z1_lb1 > 1:\n","                self.z1_lb1 = 1 \n","                \n","            if self.z2_lb1 < 0:\n","                self.z2_lb1 = 0\n","            elif self.z2_lb1 > 1:\n","                self.z2_lb1 = 1 \n","                \n","            if self.z3_lb1 < 0:\n","                self.z3_lb1 = 0\n","            elif self.z3_lb1 > 1:\n","                self.z3_lb1 = 1   \n","                \n","        elif self.fairness_type == 'dp':\n","            yhat_yz1 = {}\n","            yhat_yz2 = {}\n","            yhat_yz3 = {}\n","            yhat_y = {}\n","            \n","            ones_array = np.ones(len(self.y_data))\n","            ones_tensor = torch.FloatTensor(ones_array).cuda()\n","            dp_loss = criterion((F.tanh(logit.squeeze())+1)/2, ones_tensor.squeeze()) # Note that ones tensor puts as the true label\n","            \n","            for tmp_yz1 in self.yz1_tuple:\n","                yhat_yz1[tmp_yz1] = float(torch.sum(dp_loss[self.clean_yz1_index[tmp_yz1]])) / self.clean_z1_len[tmp_yz1[1]]\n","            \n","            for tmp_yz2 in self.yz2_tuple:\n","                yhat_yz2[tmp_yz2] = float(torch.sum(dp_loss[self.clean_yz2_index[tmp_yz2]])) / self.clean_z2_len[tmp_yz2[1]]\n","            \n","            for tmp_yz3 in self.yz3_tuple:\n","                yhat_yz3[tmp_yz3] = float(torch.sum(dp_loss[self.clean_yz3_index[tmp_yz3]])) / self.clean_z3_len[tmp_yz3[1]]\n","                    \n","            \n","            y1_diff_z1 = abs(yhat_yz1[(1, 1)] - yhat_yz1[(1, 0)])\n","            y0_diff_z1 = abs(yhat_yz1[(-1, 1)] - yhat_yz1[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z1 > y0_diff_z1:\n","                if yhat_yz1[(1, 1)] > yhat_yz1[(1, 0)]:\n","                    self.z1_lb1 += self.alpha\n","                else:\n","                    self.z1_lb1 -= self.alpha\n","            else:\n","                if yhat_yz1[(-1, 1)] > yhat_yz1[(-1, 0)]:\n","                    self.z1_lb2 += self.alpha\n","                else:\n","                    self.z1_lb2 -= self.alpha\n","            \n","            y1_diff_z2 = abs(yhat_yz2[(1, 1)] - yhat_yz2[(1, 0)])\n","            y0_diff_z2 = abs(yhat_yz2[(-1, 1)] - yhat_yz2[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z2 > y0_diff_z2:\n","                if yhat_yz2[(1, 1)] > yhat_yz2[(1, 0)]:\n","                    self.z2_lb1 += self.alpha\n","                else:\n","                    self.z2_lb1 -= self.alpha\n","            else:\n","                if yhat_yz2[(-1, 1)] > yhat_yz2[(-1, 0)]:\n","                    self.z2_lb2 += self.alpha\n","                else:\n","                    self.z2_lb2 -= self.alpha\n","                    \n","            y1_diff_z3 = abs(yhat_yz3[(1, 1)] - yhat_yz3[(1, 0)])\n","            y0_diff_z3 = abs(yhat_yz3[(-1, 1)] - yhat_yz3[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if y1_diff_z3 > y0_diff_z3:\n","                if yhat_yz3[(1, 1)] > yhat_yz3[(1, 0)]:\n","                    self.z3_lb1 += self.alpha\n","                else:\n","                    self.z3_lb1 -= self.alpha\n","            else:\n","                if yhat_yz3[(-1, 1)] > yhat_yz3[(-1, 0)]:\n","                    self.z3_lb2 += self.alpha\n","                else:\n","                    self.z3_lb2 -= self.alpha\n","                    \n","            if self.z1_lb1 < 0:\n","                self.z1_lb1 = 0\n","            elif self.z1_lb1 > 1:\n","                self.z1_lb1 = 1 \n","                \n","            if self.z2_lb1 < 0:\n","                self.z2_lb1 = 0\n","            elif self.z2_lb1 > 1:\n","                self.z2_lb1 = 1 \n","                \n","            if self.z3_lb1 < 0:\n","                self.z3_lb1 = 0\n","            elif self.z3_lb1 > 1:\n","                self.z3_lb1 = 1   \n","\n","    def select_fair_robust_sample(self):\n","        \"\"\"Selects fair and robust samples and adjusts the lambda values for fairness. \n","        See our paper for algorithm details.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        logit = self.get_logit()\n","        \n","        self.adjust_lambda(logit)\n","\n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","        profit = torch.max(loss)-loss\n","        \n","        current_weight_sum_z1 = {}\n","        current_weight_sum_z2 = {}\n","        current_weight_sum_z3 = {}\n","        \n","        lb_ratio_z1 = {}\n","        lb_ratio_z2 = {}\n","        lb_ratio_z3 = {}\n","        \n","        for tmp_yz1 in self.yz1_tuple:\n","            if tmp_yz1 == (1,1):\n","                lb_ratio_z1[tmp_yz1] = self.z1_lb1\n","            elif tmp_yz1 == (1,0): \n","                lb_ratio_z1[tmp_yz1] = 1-self.z1_lb1\n","            elif tmp_yz1 == (-1,1):\n","                lb_ratio_z1[tmp_yz1] = self.z1_lb2\n","            elif tmp_yz1 == (-1,0):\n","                lb_ratio_z1[tmp_yz1] = 1-self.z1_lb2\n","            \n","            current_weight_sum_z1[tmp_yz1] = 0\n","        \n","        for tmp_yz2 in self.yz2_tuple:\n","            if tmp_yz2 == (1,1):\n","                lb_ratio_z2[tmp_yz2] = self.z2_lb1\n","            elif tmp_yz2 == (1,0): \n","                lb_ratio_z2[tmp_yz2] = 1-self.z2_lb1\n","            elif tmp_yz2 == (-1,1):\n","                lb_ratio_z2[tmp_yz2] = self.z2_lb2\n","            elif tmp_yz2 == (-1,0):\n","                lb_ratio_z2[tmp_yz2] = 1-self.z2_lb2\n","            \n","            current_weight_sum_z2[tmp_yz2] = 0\n","        \n","        for tmp_yz3 in self.yz3_tuple:\n","            if tmp_yz3 == (1,1):\n","                lb_ratio_z3[tmp_yz3] = self.z3_lb1\n","            elif tmp_yz3 == (1,0): \n","                lb_ratio_z3[tmp_yz3] = 1-self.z3_lb1\n","            elif tmp_yz3 == (-1,1):\n","                lb_ratio_z3[tmp_yz3] = self.z3_lb2\n","            elif tmp_yz3 == (-1,0):\n","                lb_ratio_z3[tmp_yz3] = 1-self.z3_lb2\n","            \n","            current_weight_sum_z3[tmp_yz3] = 0\n","        \n","        # Greedy-based algorithm\n","        \n","        (_, sorted_index) = torch.topk(profit, len(profit), largest=True, sorted=True)\n","        \n","        clean_index = []\n","        \n","        total_selected = 0\n","        \n","        desired_size = int(self.tau * len(self.y_data))\n","        \n","        for j in sorted_index:\n","            tmp_y = self.y_data[j].item()\n","            tmp_z1 = self.z1_data[j].item()\n","            tmp_z2 = self.z2_data[j].item()\n","            tmp_z3 = self.z3_data[j].item()\n","            current_weight_list_z1 = list(current_weight_sum_z1.values())\n","            current_weight_list_z2 = list(current_weight_sum_z2.values())\n","            current_weight_list_z3 = list(current_weight_sum_z3.values())\n","            if total_selected >= desired_size:\n","                break\n","            if all(i < desired_size for i in current_weight_list_z1) and all(i < desired_size for i in current_weight_list_z2) and all(i < desired_size for i in current_weight_list_z3):\n","                clean_index.append(j)\n","                \n","                current_weight_sum_z1[(tmp_y, tmp_z1)] += 2 - lb_ratio_z1[(tmp_y, tmp_z1)]\n","                current_weight_sum_z1[(tmp_y, 1-tmp_z1)] += 1 - lb_ratio_z1[(tmp_y, 1-tmp_z1)]\n","                current_weight_sum_z1[(tmp_y * -1, tmp_z1)] += 1\n","                current_weight_sum_z1[(tmp_y * -1, 1-tmp_z1)] += 1\n","                \n","                current_weight_sum_z2[(tmp_y, tmp_z2)] += 2 - lb_ratio_z2[(tmp_y, tmp_z2)]\n","                current_weight_sum_z2[(tmp_y, 1-tmp_z2)] += 1 - lb_ratio_z2[(tmp_y, 1-tmp_z2)]\n","                current_weight_sum_z2[(tmp_y * -1, tmp_z2)] += 1\n","                current_weight_sum_z2[(tmp_y * -1, 1-tmp_z2)] += 1\n","                \n","                current_weight_sum_z3[(tmp_y, tmp_z3)] += 2 - lb_ratio_z3[(tmp_y, tmp_z3)]\n","                current_weight_sum_z3[(tmp_y, 1-tmp_z3)] += 1 - lb_ratio_z3[(tmp_y, 1-tmp_z3)]\n","                current_weight_sum_z3[(tmp_y * -1, tmp_z3)] += 1\n","                current_weight_sum_z3[(tmp_y * -1, 1-tmp_z3)] += 1\n","                \n","                total_selected += 1        \n","        \n","        clean_index = torch.LongTensor(clean_index).cuda()\n","        \n","        self.batch_num = int(len(clean_index)/self.batch_size)\n","        \n","        # Update the variables\n","        self.clean_index = clean_index\n","        \n","        for tmp_z1 in self.z1_item:\n","            combined = torch.cat((self.z1_index[tmp_z1], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_z1_index[tmp_z1] = intersection \n","        \n","        for tmp_z2 in self.z2_item:\n","            combined = torch.cat((self.z2_index[tmp_z2], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_z2_index[tmp_z2] = intersection \n","        \n","        for tmp_z3 in self.z3_item:\n","            combined = torch.cat((self.z3_index[tmp_z3], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_z3_index[tmp_z3] = intersection \n","            \n","        for tmp_y in self.y_item:\n","            combined = torch.cat((self.y_index[tmp_y], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_y_index[tmp_y] = intersection\n","        \n","        for tmp_yz1 in self.yz1_tuple:\n","            combined = torch.cat((self.yz1_index[tmp_yz1], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_yz1_index[tmp_yz1] = intersection\n","            \n","        for tmp_yz2 in self.yz2_tuple:\n","            combined = torch.cat((self.yz2_index[tmp_yz2], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_yz2_index[tmp_yz2] = intersection\n","            \n","        for tmp_yz3 in self.yz3_tuple:\n","            combined = torch.cat((self.yz3_index[tmp_yz3], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_yz3_index[tmp_yz3] = intersection\n","        \n","        \n","        for tmp_z1 in self.z1_item:\n","            self.clean_z1_len[tmp_z1] = len(self.clean_z1_index[tmp_z1])\n","        \n","        for tmp_z2 in self.z2_item:\n","            self.clean_z2_len[tmp_z2] = len(self.clean_z2_index[tmp_z2])\n","        \n","        for tmp_z3 in self.z3_item:\n","            self.clean_z3_len[tmp_z3] = len(self.clean_z3_index[tmp_z3])\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","        \n","        for tmp_yz1 in self.yz1_tuple:\n","            self.clean_yz1_len[tmp_yz1] = len(self.clean_yz1_index[tmp_yz1])\n","        \n","        for tmp_yz2 in self.yz2_tuple:\n","            self.clean_yz2_len[tmp_yz2] = len(self.clean_yz2_index[tmp_yz2])\n","        \n","        for tmp_yz3 in self.yz2_tuple:\n","            self.clean_yz3_len[tmp_yz3] = len(self.clean_yz3_index[tmp_yz3])\n","            \n","        \n","        return clean_index\n","        \n","    \n","    def select_batch_replacement(self, batch_size, full_index, batch_num, replacement = False, weight = None):\n","        \"\"\"Selects a certain number of batches based on the given batch size.\n","        \n","        Args: \n","            batch_size: An integer for the data size in a batch.\n","            full_index: An array containing the candidate data indices.\n","            batch_num: An integer indicating the number of batches.\n","            replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        select_index = []\n","        \n","        if replacement == True:\n","            for _ in range(batch_num):\n","                if weight == None:\n","                    weight_norm = weight/torch.sum(weight)\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False, p = weight_norm))\n","                else:\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False))\n","        else:\n","            tmp_index = full_index.detach().cpu().numpy().copy()\n","            random.shuffle(tmp_index)\n","            \n","            start_idx = 0\n","            for i in range(batch_num):\n","                if start_idx + batch_size > len(full_index):\n","                    select_index.append(np.concatenate((tmp_index[start_idx:], tmp_index[ : batch_size - (len(full_index)-start_idx)])))\n","                    \n","                    start_idx = len(full_index)-start_idx\n","                else:\n","\n","                    select_index.append(tmp_index[start_idx:start_idx + batch_size])\n","                    start_idx += batch_size\n","            \n","        return select_index\n","\n","\n","    \n","    def decide_fair_batch_size(self):\n","        \"\"\"Calculates each class size based on the lambda values (lb1 and lb2) for fairness.\n","        \n","        Returns:\n","            Each class size for fairness.\n","            \n","        \"\"\"\n","        \n","        each_size_z1 = {}\n","        each_size_z2 = {}\n","        each_size_z3 = {}\n","\n","        for tmp_yz1 in self.yz1_tuple:\n","            self.S1[tmp_yz1] = self.batch_size * (self.clean_yz1_len[tmp_yz1])/len(self.clean_index)\n","            \n","        for tmp_yz2 in self.yz2_tuple:\n","            self.S2[tmp_yz2] = self.batch_size * (self.clean_yz2_len[tmp_yz2])/len(self.clean_index)\n","        \n","        for tmp_yz3 in self.yz3_tuple:\n","            self.S3[tmp_yz3] = self.batch_size * (self.clean_yz3_len[tmp_yz3])/len(self.clean_index)\n","\n","        # Based on the updated lambdas, determine the size of each class in a batch\n","        if self.fairness_type == 'eqopp':\n","            # lb1 * loss_z1 + (1-lb1) * loss_z0\n","\n","            each_size_z1[(1,1)] = round(self.z1_lb1 * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(1,0)] = round((1-self.z1_lb1) * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(-1,1)] = round(self.S1[(-1,1)])\n","            each_size_z1[(-1,0)] = round(self.S1[(-1,0)])\n","            \n","            each_size_z2[(1,1)] = round(self.z2_lb1 * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(1,0)] = round((1-self.z2_lb1) * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(-1,1)] = round(self.S2[(-1,1)])\n","            each_size_z2[(-1,0)] = round(self.S2[(-1,0)])\n","            \n","            each_size_z3[(1,1)] = round(self.z3_lb1 * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(1,0)] = round((1-self.z3_lb1) * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(-1,1)] = round(self.S3[(-1,1)])\n","            each_size_z3[(-1,0)] = round(self.S3[(-1,0)])\n","\n","        elif self.fairness_type == 'eqodds':\n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","\n","            each_size_z1[(1,1)] = round(self.z1_lb1 * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(1,0)] = round((1-self.z1_lb1) * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(-1,1)] = round(self.z1_lb2 * (self.S1[(-1,1)] + self.S1[(-1,0)]))\n","            each_size_z1[(-1,0)] = round((1-self.z1_lb2) * (self.S1[(-1,1)] + self.S1[(-1,0)]))\n","            \n","            each_size_z2[(1,1)] = round(self.z2_lb1 * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(1,0)] = round((1-self.z2_lb1) * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(-1,1)] = round(self.z2_lb2 * (self.S2[(-1,1)] + self.S2[(-1,0)]))\n","            each_size_z2[(-1,0)] = round((1-self.z2_lb2) * (self.S2[(-1,1)] + self.S2[(-1,0)]))\n","            \n","            each_size_z3[(1,1)] = round(self.z3_lb1 * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(1,0)] = round((1-self.z3_lb1) * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(-1,1)] = round(self.z3_lb2 * (self.S3[(-1,1)] + self.S3[(-1,0)]))\n","            each_size_z3[(-1,0)] = round((1-self.z3_lb2) * (self.S3[(-1,1)] + self.S3[(-1,0)]))\n","\n","        elif self.fairness_type == 'dp':\n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","\n","            each_size_z1[(1,1)] = round(self.z1_lb1 * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(1,0)] = round((1-self.z1_lb1) * (self.S1[(1,1)] + self.S1[(1,0)]))\n","            each_size_z1[(-1,1)] = round(self.z1_lb2 * (self.S1[(-1,1)] + self.S1[(-1,0)]))\n","            each_size_z1[(-1,0)] = round((1-self.z1_lb2) * (self.S1[(-1,1)] + self.S1[(-1,0)]))\n","            \n","            each_size_z2[(1,1)] = round(self.z2_lb1 * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(1,0)] = round((1-self.z2_lb1) * (self.S2[(1,1)] + self.S2[(1,0)]))\n","            each_size_z2[(-1,1)] = round(self.z2_lb2 * (self.S2[(-1,1)] + self.S2[(-1,0)]))\n","            each_size_z2[(-1,0)] = round((1-self.z2_lb2) * (self.S2[(-1,1)] + self.S2[(-1,0)]))\n","            \n","            each_size_z3[(1,1)] = round(self.z3_lb1 * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(1,0)] = round((1-self.z3_lb1) * (self.S3[(1,1)] + self.S3[(1,0)]))\n","            each_size_z3[(-1,1)] = round(self.z3_lb2 * (self.S3[(-1,1)] + self.S3[(-1,0)]))\n","            each_size_z3[(-1,0)] = round((1-self.z3_lb2) * (self.S3[(-1,1)] + self.S3[(-1,0)]))\n","        \n","        return each_size_z1, each_size_z2, each_size_z3\n","        \n","        \n","    \n","    def __iter__(self):\n","        \"\"\"Iters the full process of fair and robust sample selection for serving the batches to training.\n","        \n","        Returns:\n","            Indexes that indicate the data in each batch.\n","            \n","        \"\"\"\n","        self.count_epoch += 1\n","        \n","        if self.count_epoch > self.warm_start:\n","\n","            _ = self.select_fair_robust_sample()\n","\n","\n","            each_size_z1, each_size_z2, each_size_z3 = self.decide_fair_batch_size()\n","\n","            # Get the indices for each class\n","            sort_index_y_1_z1_1 = self.select_batch_replacement(each_size_z1[(1, 1)], self.clean_yz1_index[(1,1)], self.batch_num, self.replacement)\n","            sort_index_y_0_z1_1 = self.select_batch_replacement(each_size_z1[(-1, 1)], self.clean_yz1_index[(-1,1)], self.batch_num, self.replacement)\n","            sort_index_y_1_z1_0 = self.select_batch_replacement(each_size_z1[(1, 0)], self.clean_yz1_index[(1,0)], self.batch_num, self.replacement)\n","            sort_index_y_0_z1_0 = self.select_batch_replacement(each_size_z1[(-1, 0)], self.clean_yz1_index[(-1,0)], self.batch_num, self.replacement)\n","            \n","            sort_index_y_1_z2_1 = self.select_batch_replacement(each_size_z2[(1, 1)], self.clean_yz2_index[(1,1)], self.batch_num, self.replacement)\n","            sort_index_y_0_z2_1 = self.select_batch_replacement(each_size_z2[(-1, 1)], self.clean_yz2_index[(-1,1)], self.batch_num, self.replacement)\n","            sort_index_y_1_z2_0 = self.select_batch_replacement(each_size_z2[(1, 0)], self.clean_yz2_index[(1,0)], self.batch_num, self.replacement)\n","            sort_index_y_0_z2_0 = self.select_batch_replacement(each_size_z2[(-1, 0)], self.clean_yz2_index[(-1,0)], self.batch_num, self.replacement)\n","            \n","            sort_index_y_1_z3_1 = self.select_batch_replacement(each_size_z3[(1, 1)], self.clean_yz3_index[(1,1)], self.batch_num, self.replacement)\n","            sort_index_y_0_z3_1 = self.select_batch_replacement(each_size_z3[(-1, 1)], self.clean_yz3_index[(-1,1)], self.batch_num, self.replacement)\n","            sort_index_y_1_z3_0 = self.select_batch_replacement(each_size_z3[(1, 0)], self.clean_yz3_index[(1,0)], self.batch_num, self.replacement)\n","            sort_index_y_0_z3_0 = self.select_batch_replacement(each_size_z3[(-1, 0)], self.clean_yz3_index[(-1,0)], self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                key_in_fairbatch_z1 = sort_index_y_0_z1_0[i].copy()\n","                key_in_fairbatch_z1 = np.hstack((key_in_fairbatch_z1, sort_index_y_1_z1_0[i].copy()))\n","                key_in_fairbatch_z1 = np.hstack((key_in_fairbatch_z1, sort_index_y_0_z1_1[i].copy()))\n","                key_in_fairbatch_z1 = np.hstack((key_in_fairbatch_z1, sort_index_y_1_z1_1[i].copy()))\n","\n","                random.shuffle(key_in_fairbatch_z1)\n","\n","                yield key_in_fairbatch_z1\n","                \n","                key_in_fairbatch_z2 = sort_index_y_0_z2_0[i].copy()\n","                key_in_fairbatch_z2 = np.hstack((key_in_fairbatch_z2, sort_index_y_1_z2_0[i].copy()))\n","                key_in_fairbatch_z2 = np.hstack((key_in_fairbatch_z2, sort_index_y_0_z2_1[i].copy()))\n","                key_in_fairbatch_z2 = np.hstack((key_in_fairbatch_z2, sort_index_y_1_z2_1[i].copy()))\n","\n","                random.shuffle(key_in_fairbatch_z2)\n","\n","                yield key_in_fairbatch_z2\n","                \n","                key_in_fairbatch_z3 = sort_index_y_0_z3_0[i].copy()\n","                key_in_fairbatch_z3 = np.hstack((key_in_fairbatch_z3, sort_index_y_1_z3_0[i].copy()))\n","                key_in_fairbatch_z3 = np.hstack((key_in_fairbatch_z3, sort_index_y_0_z3_1[i].copy()))\n","                key_in_fairbatch_z3 = np.hstack((key_in_fairbatch_z3, sort_index_y_1_z3_1[i].copy()))\n","\n","                random.shuffle(key_in_fairbatch_z3)\n","\n","                yield key_in_fairbatch_z3\n","\n","        else:\n","            entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","\n","            sort_index = self.select_batch_replacement(self.batch_size, entire_index, self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                yield sort_index[i]\n","        \n","                               \n","\n","    def __len__(self):\n","        \"\"\"Returns the length of data.\"\"\"\n","        \n","        return len(self.y_data)\n","\n","class FairRobust2(Sampler):\n","    \"\"\"FairRobust (Sampler in DataLoader).\n","    \n","    This class is for implementing the lambda adjustment and batch selection of FairBatch [Roh et al., ICLR 2021] with robust training.\n","\n","    Attributes:\n","        model: A model containing the intermediate states of the training.\n","        x_, y_, z_data: Tensor-based train data.\n","        alpha: A positive number for step size that used in the lambda adjustment.\n","        fairness_type: A string indicating the target fairness type \n","                       among original, demographic parity (dp), equal opportunity (eqopp), and equalized odds (eqodds).\n","        replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        N: An integer counting the size of data.\n","        batch_size: An integer for the size of a batch.\n","        batch_num: An integer for total number of batches in an epoch.\n","        y_, z_item: Lists that contains the unique values of the y_data and z_data, respectively.\n","        yz_tuple: Lists for pairs of y_item and z_item.\n","        y_, z_, yz_mask: Dictionaries utilizing as array masks.\n","        y_, z_, yz_index: Dictionaries containing the indexes of each class.\n","        y_, z_, yz_len: Dictionaries containing the length information.\n","        clean_index: A list that contains the data indexes of selected samples.\n","        clean_y_, clean_z_, clean_yz_index: Dictionaries containing the indexes of each class in the selected set.\n","        clean_y_, clean_z_, clean_yz_len: Dictionaries containing the length information in the selected set.\n","        S: A dictionary containing the default size of each class in a batch.\n","        lb1, lb2: (0~1) real numbers indicating the lambda values for fairness [Roh et al., ICLR 2021].\n","        tau: (0~1) real number indicating the clean ratio of the data.\n","        warm_start: An integer for warm-start period.\n","\n","        \n","    \"\"\"\n","    def __init__(self, model, x_tensor, y_tensor, z1_tensor, z2_tensor, z3_tensor, target_fairness, parameters, replacement = False, seed = 0):\n","        \"\"\"Initializes FairBatch.\"\"\"\n","        \n","        self.model = model\n","        \n","        np.random.seed(seed)\n","        random.seed(seed)\n","        \n","        self.x_data = x_tensor\n","        self.y_data = y_tensor\n","        self.z1_data = z1_tensor\n","        self.z2_data = z2_tensor\n","        self.z3_data = z3_tensor\n","        \n","        #alpha: how to modify rate of each category\n","        self.alpha = parameters.alpha\n","        self.fairness_type = target_fairness\n","        \n","        self.replacement = replacement\n","        # N: number of elements\n","        self.N = len(z1_tensor)\n","        \n","        self.batch_size = parameters.batch_size\n","        self.batch_num = int(len(self.y_data) / self.batch_size)\n","        \n","        # Takes the unique values of the tensors\n","        self.z_item = [tuple((0,0,0)), tuple((0,0,1)), tuple((0,1,0)), tuple((0,1,1)), tuple((1,0,0)), tuple((1,0,1)), tuple((1,1,0)), tuple((1,1,1))]\n","        self.y_item = list(set(y_tensor.tolist()))\n","        # yz_tuple: (-1,0), (-1,1), (1,1), (1,0)\n","        self.yz_tuple = list(itertools.product(self.y_item, self.z_item))\n","        \n","        # Makes masks\n","        #z_mask: z = 1 or z = 0 (list)\n","        self.z_mask = {}\n","        #z_mask: y = 1 or y = -1 (list)\n","        self.y_mask = {}\n","        self.yz_mask = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.z_mask[tmp_z] = (self.z1_data == tmp_z[0])&(self.z2_data == tmp_z[1])&(self.z3_data == tmp_z[2])\n","            \n","        for tmp_y in self.y_item:\n","            self.y_mask[tmp_y] = (self.y_data == tmp_y)\n","            \n","        for tmp_yz in self.yz_tuple:\n","            self.yz_mask[tmp_yz] = (self.y_data == tmp_yz[0]) & (self.z1_data == tmp_yz[1][0])&(self.z2_data == tmp_yz[1][1])&(self.z3_data == tmp_yz[1][2])\n","        \n","\n","        # Finds the index\n","        self.z_index = {}\n","        self.y_index = {}\n","        self.yz_index = {}\n","        #index of z=1, z=0\n","        for tmp_z in self.z_item:\n","            self.z_index[tmp_z] = (self.z_mask[tmp_z] == 1).nonzero().squeeze()\n","        #index of y=-1, y=0\n","        for tmp_y in self.y_item:\n","            self.y_index[tmp_y] = (self.y_mask[tmp_y] == 1).nonzero().squeeze()\n","        #index of (1,1); (-1,1); (1,0); (-1,0)\n","        for tmp_yz in self.yz_tuple:\n","            self.yz_index[tmp_yz] = (self.yz_mask[tmp_yz] == 1).nonzero().squeeze()\n","            \n","        self.entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","            \n","        # Length information\n","        self.z_len = {}\n","        self.y_len = {}\n","        self.yz_len = {}\n","        \n","        for tmp_z in self.z_item:\n","            self.z_len[tmp_z] = len(self.z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            self.y_len[tmp_y] = len(self.y_index[tmp_y])\n","            \n","        for tmp_yz in self.yz_tuple:\n","            self.yz_len[tmp_yz] = len(self.yz_index[tmp_yz])\n","\n","        # Default batch size\n","        self.S = {}\n","        \n","        # len of each (y,z)\n","        for tmp_yz in self.yz_tuple:\n","            self.S[tmp_yz] = self.batch_size * (self.yz_len[tmp_yz])/self.N\n","            \n","        summ1 = 0\n","        summ2 = 0\n","        for tmp_yz in self.yz_tuple:\n","            if tmp_yz[0] == 1:\n","                summ1 += self.S[tmp_yz]\n","            else:\n","                summ2 += self.S[tmp_yz]\n","        # lambda = (y=1,z=1)/(y=1)\n","        self.lb1_1 = (self.S[1.0, (0, 0, 0)])/summ1\n","        self.lb1_2 = (self.S[1.0, (0, 0, 1)])/summ1\n","        self.lb1_3 = (self.S[1.0, (0, 1, 0)])/summ1\n","        self.lb1_4 = (self.S[1.0, (0, 1, 1)])/summ1\n","        self.lb1_5 = (self.S[1.0, (1, 0, 0)])/summ1\n","        self.lb1_6 = (self.S[1.0, (1, 0, 1)])/summ1\n","        self.lb1_7 = (self.S[1.0, (1, 1, 0)])/summ1\n","        \n","        \n","        # (y=-1,z=1)/(y=-1)\n","        self.lb2_1 = (self.S[-1.0, (0, 0, 0)])/summ2\n","        self.lb2_2 = (self.S[-1.0, (0, 0, 1)])/summ2\n","        self.lb2_3 = (self.S[-1.0, (0, 1, 0)])/summ2\n","        self.lb2_4 = (self.S[-1.0, (0, 1, 1)])/summ2\n","        self.lb2_5 = (self.S[-1.0, (1, 0, 0)])/summ2\n","        self.lb2_6 = (self.S[-1.0, (1, 0, 1)])/summ2\n","        self.lb2_7 = (self.S[-1.0, (1, 1, 0)])/summ2\n","        \n","        # For cleanselection parameters\n","        self.tau = parameters.tau # Clean ratio\n","        self.warm_start = parameters.warm_start\n","    \n","        self.count_epoch = 0\n","        \n","            \n","        # Clean sample selection\n","        self.clean_index = np.arange(0,len(self.y_data))\n","        \n","        # Finds the index\n","        self.clean_z_index = {}\n","        self.clean_y_index = {}\n","        self.clean_yz_index = {}\n","        # index of each z values in list\n","        for tmp_z in self.z_item:\n","            self.clean_z_index[tmp_z] = (self.z_mask[tmp_z] == 1)[self.clean_index].nonzero().squeeze()\n","        # index of each y values in list\n","        for tmp_y in self.y_item:\n","            self.clean_y_index[tmp_y] = (self.y_mask[tmp_y] == 1)[self.clean_index].nonzero().squeeze()\n","        # index of each yz values in list\n","        for tmp_yz in self.yz_tuple:\n","            self.clean_yz_index[tmp_yz] = (self.yz_mask[tmp_yz] == 1)[self.clean_index].nonzero().squeeze()\n","        \n","       # Length information\n","        self.clean_z_len = {}\n","        self.clean_y_len = {}\n","        self.clean_yz_len = {}\n","        \n","        for tmp_z in self.z_item:\n","            \n","            self.clean_z_len[tmp_z] = len(self.clean_z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            \n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","            \n","        for tmp_yz in self.yz_tuple:\n","            \n","            self.clean_yz_len[tmp_yz] = len(self.clean_yz_index[tmp_yz])\n"," \n","      \n","    def get_logit(self):\n","        \"\"\"Runs forward pass of the intermediate model with the training data.\n","        \n","        Returns:\n","            Outputs (logits) of the model.\n","\n","        \"\"\"\n","        \n","        self.model.eval()\n","        logit = self.model(self.x_data)\n","        \n","        return logit\n","    \n","    \n","    def adjust_lambda(self, logit):\n","        \"\"\"Adjusts the lambda values using FairBatch [Roh et al., ICLR 2021].\n","        See our paper for algorithm details.\n","        \n","        Args: \n","            logit: A torch tensor that contains the intermediate model's output on the training data.\n","        \n","        \"\"\"\n","        \n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        \n","        if self.fairness_type == 'eqopp':\n","        \n","            yhat_yz = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz in self.yz_tuple:\n","                \n","                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.clean_yz_index[tmp_yz]])) / self.clean_yz_len[tmp_yz]\n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / self.clean_y_len[tmp_y]\n","            \n","            # lb1 * loss_z1 + (1-lb1) * loss_z0\n","            summ_1 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == 1:\n","                    summ_1 += yhat_yz[tmp_yz]\n","            avg = summ_1/8\n","\n","            if yhat_yz[(1.0, (0, 0, 0))] > avg:\n","                self.lb1_1 += self.alpha\n","                if self.lb1_1 > 1:\n","                    self.lb1_1 = 1\n","            else:\n","                self.lb1_1 -= self.alpha\n","                if self.lb1_1 < 0:\n","                    self.lb1_1 = 0\n","            \n","            if yhat_yz[(1.0, (0, 0, 1))] > avg:\n","                self.lb1_2 += self.alpha\n","                if self.lb1_2 > 1:\n","                    self.lb1_2 = 1\n","            else:\n","                self.lb1_2 -= self.alpha\n","                if self.lb1_2 < 0:\n","                    self.lb1_2 = 0\n","            \n","            if yhat_yz[(1.0, (0, 1, 0))] > avg:\n","                \n","                self.lb1_3 += self.alpha\n","                if self.lb1_3 > 1:\n","                    self.lb1_3 = 1\n","            else:\n","                self.lb1_3 -= self.alpha\n","                \n","                if self.lb1_3 < 0:\n","                    self.lb1_3 = 0\n","            \n","            if yhat_yz[(1.0, (0, 1, 1))] > avg:\n","                self.lb1_4 += self.alpha\n","                if self.lb1_4 > 1:\n","                    self.lb1_4 = 1\n","            else:\n","                self.lb1_4 -= self.alpha\n","                if self.lb1_4 < 0:\n","                    self.lb1_4 = 0\n","            \n","            if yhat_yz[(1.0, (1, 0, 0))] > avg:\n","                self.lb1_5 += self.alpha\n","                if self.lb1_5 > 1:\n","                    self.lb1_5 = 1\n","            else:\n","                self.lb1_5 -= self.alpha\n","                if self.lb1_5 < 0:\n","                    self.lb1_5 = 0\n","            \n","            if yhat_yz[(1.0, (1, 0, 1))] > avg:\n","                self.lb1_6 += self.alpha\n","                if self.lb1_6 > 1:\n","                    self.lb1_6 = 1\n","            else:\n","                self.lb1_6 -= self.alpha\n","                if self.lb1_6 < 0:\n","                    self.lb1_6 = 0\n","            \n","            if yhat_yz[(1.0, (1, 1, 0))] > avg:\n","                self.lb1_7 += self.alpha\n","                if self.lb1_7 > 1:\n","                    self.lb1_7 = 1\n","            else:\n","                self.lb1_7 -= self.alpha\n","                if self.lb1_7 < 0:\n","                    self.lb1_7 = 0\n","                \n","            if self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 > 1:\n","                minus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 1)/7\n","                self.lb1_1 -= minus\n","                self.lb1_2 -= minus\n","                self.lb1_3 -= minus\n","                self.lb1_4 -= minus\n","                self.lb1_5 -= minus\n","                self.lb1_6 -= minus\n","                self.lb1_7 -= minus\n","            \n","            elif self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 < 0:\n","                plus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 0)/7\n","                self.lb1_1 -= plus\n","                self.lb1_2 -= plus\n","                self.lb1_3 -= plus\n","                self.lb1_4 -= plus\n","                self.lb1_5 -= plus\n","                self.lb1_6 -= plus\n","                self.lb1_7 -= plus\n","                \n","        elif self.fairness_type == 'eqodds':\n","            \n","            yhat_yz = {}\n","            yhat_y = {}\n","                        \n","            eo_loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","            \n","            for tmp_yz in self.yz_tuple:\n","                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.clean_yz_index[tmp_yz]])) / (self.clean_yz_len[tmp_yz]+1)\n","                \n","                \n","            for tmp_y in self.y_item:\n","                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.clean_y_index[tmp_y]])) / (self.clean_y_len[tmp_y]+1)\n","            summ_1 = 0\n","            \n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == 1:\n","                    \n","                    summ_1 += yhat_yz[tmp_yz]\n","            avg_1 = summ_1/8\n","            summ_2 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == -1:\n","                    summ_2 += yhat_yz[tmp_yz]\n","            avg_2 = summ_2/8\n","            std_1 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == 1:\n","                    std_1 += abs(yhat_yz[tmp_yz]-avg_1)\n","            std_1 /= 8\n","            std_2 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == -1:\n","                    std_2 += abs(yhat_yz[tmp_yz]-avg_2)\n","            std_2 /= 8\n","#             y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n","#             y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if std_1 > std_2:\n","                if yhat_yz[(1.0, (0, 0, 0))] > avg_1:\n","                    self.lb1_1 += self.alpha\n","                    if self.lb1_1 > 1:\n","                        self.lb1_1 = 1\n","                else:\n","                    self.lb1_1 -= self.alpha\n","                    if self.lb1_1 < 0:\n","                        self.lb1_1 = 0\n","\n","                if yhat_yz[(1.0, (0, 0, 1))] > avg_1:\n","                    self.lb1_2 += self.alpha\n","                    if self.lb1_2 > 1:\n","                        self.lb1_2 = 1\n","                else:\n","                    self.lb1_2 -= self.alpha\n","                    if self.lb1_2 < 0:\n","                        self.lb1_2 = 0\n","\n","                if yhat_yz[(1.0, (0, 1, 0))] > avg_1:\n","                    self.lb1_3 += self.alpha\n","                    if self.lb1_3 > 1:\n","                        self.lb1_3 = 1\n","                else:\n","                    self.lb1_3 -= self.alpha\n","                    if self.lb1_3 < 0:\n","                        self.lb1_3 = 0\n","\n","                if yhat_yz[(1.0, (0, 1, 1))] > avg_1:\n","                    self.lb1_4 += self.alpha\n","                    if self.lb1_4 > 1:\n","                        self.lb1_4 = 1\n","                else:\n","                    self.lb1_4 -= self.alpha\n","                    if self.lb1_4 < 0:\n","                        self.lb1_4 = 0\n","\n","                if yhat_yz[(1.0, (1, 0, 0))] > avg_1:\n","                    self.lb1_5 += self.alpha\n","                    if self.lb1_5 > 1:\n","                        self.lb1_5 = 1\n","                else:\n","                    self.lb1_5 -= self.alpha\n","                    if self.lb1_5 < 0:\n","                        self.lb1_5 = 0\n","\n","                if yhat_yz[(1.0, (1, 0, 1))] > avg_1:\n","                    self.lb1_6 += self.alpha\n","                    if self.lb1_6 > 1:\n","                        self.lb1_6 = 1\n","                else:\n","                    self.lb1_6 -= self.alpha\n","                    if self.lb1_6 < 0:\n","                        self.lb1_6 = 0\n","\n","                if yhat_yz[(1.0, (1, 1, 0))] > avg_1:\n","                    self.lb1_7 += self.alpha\n","                    if self.lb1_7 > 1:\n","                        self.lb1_7 = 1\n","                else:\n","                    self.lb1_7 -= self.alpha\n","                    if self.lb1_7 < 0:\n","                        self.lb1_7 = 0\n","                    # if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","                    #     self.lb1 += self.alpha\n","                    # else:\n","                    #     self.lb1 -= self.alpha\n","            else:\n","                if yhat_yz[(-1.0, (0, 0, 0))] > avg_2:\n","                    self.lb2_1 += self.alpha\n","                    if self.lb2_1 > 1:\n","                        self.lb2_1 = 1\n","                else:\n","                    self.lb2_1 -= self.alpha\n","                    if self.lb2_1 < 0:\n","                        self.lb2_1 = 0\n","\n","                if yhat_yz[(-1.0, (0, 0, 1))] > avg_2:\n","                    self.lb2_2 += self.alpha\n","                    if self.lb2_2 > 1:\n","                        self.lb2_2 = 1\n","                else:\n","                    self.lb2_2 -= self.alpha\n","                    if self.lb2_2 < 0:\n","                        self.lb2_2 = 0\n","\n","                if yhat_yz[(-1.0, (0, 1, 0))] > avg_2:\n","                    self.lb2_3 += self.alpha\n","                    if self.lb2_3 > 1:\n","                        self.lb2_3 = 1\n","                else:\n","                    self.lb2_3 -= self.alpha\n","                    if self.lb2_3 < 0:\n","                        self.lb2_3 = 0\n","\n","                if yhat_yz[(-1.0, (0, 1, 1))] > avg_2:\n","                    self.lb2_4 += self.alpha\n","                    if self.lb2_4 > 1:\n","                        self.lb2_4 = 1\n","                else:\n","                    self.lb2_4 -= self.alpha\n","                    if self.lb2_4 < 0:\n","                        self.lb2_4 = 0\n","\n","                if yhat_yz[(-1.0, (1, 0, 0))] > avg_2:\n","                    self.lb2_5 += self.alpha\n","                    if self.lb2_5 > 1:\n","                        self.lb2_5 = 1\n","                else:\n","                    self.lb2_5 -= self.alpha\n","                    if self.lb2_5 < 0:\n","                        self.lb2_5 = 0\n","\n","                if yhat_yz[(-1.0, (1, 0, 1))] > avg_2:\n","                    self.lb2_6 += self.alpha\n","                    if self.lb2_6 > 1:\n","                        self.lb2_6 = 1\n","                else:\n","                    self.lb2_6 -= self.alpha\n","                    if self.lb2_6 < 0:\n","                        self.lb2_6 = 0\n","\n","                if yhat_yz[(-1.0, (1, 1, 0))] > avg_2:\n","                    self.lb2_7 += self.alpha\n","                    if self.lb2_7 > 1:\n","                        self.lb2_7 = 1\n","                else:\n","                    self.lb2_7 -= self.alpha\n","                    if self.lb2_7 < 0:\n","                        \n","                        self.lb2_7 = 0\n","                    \n","                \n","            if self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 > 1:\n","                \n","                minus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 1)/7\n","                self.lb1_1 -= minus\n","                self.lb1_2 -= minus\n","                self.lb1_3 -= minus\n","                self.lb1_4 -= minus\n","                self.lb1_5 -= minus\n","                self.lb1_6 -= minus\n","                self.lb1_7 -= minus\n","            \n","            elif self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 < 0:\n","                plus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 0)/7\n","                self.lb1_1 -= plus\n","                self.lb1_2 -= plus\n","                self.lb1_3 -= plus\n","                self.lb1_4 -= plus\n","                self.lb1_5 -= plus\n","                self.lb1_6 -= plus\n","                self.lb1_7 -= plus\n","            \n","            if self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 > 1:\n","                minus = (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 - 1)/7\n","                self.lb2_1 -= minus\n","                self.lb2_2 -= minus\n","                self.lb2_3 -= minus\n","                self.lb2_4 -= minus\n","                self.lb2_5 -= minus\n","                self.lb2_6 -= minus\n","                self.lb2_7 -= minus\n","            \n","            elif self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 < 0:\n","                plus = (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 - 0)/7\n","                self.lb2_1 -= plus\n","                self.lb2_2 -= plus\n","                self.lb2_3 -= plus\n","                self.lb2_4 -= plus\n","                self.lb2_5 -= plus\n","                self.lb2_6 -= plus\n","                self.lb2_7 -= plus\n","                \n","        elif self.fairness_type == 'dp':\n","            yhat_yz = {}\n","            yhat_y = {}\n","            \n","            ones_array = np.ones(len(self.y_data))\n","            ones_tensor = torch.FloatTensor(ones_array).cuda()\n","            dp_loss = criterion((F.tanh(logit.squeeze())+1)/2, ones_tensor.squeeze()) # Note that ones tensor puts as the true label\n","            \n","            for tmp_yz in self.yz_tuple:\n","                \n","                yhat_yz[tmp_yz] = float(torch.sum(dp_loss[self.clean_yz_index[tmp_yz]])) / self.clean_z_len[tmp_yz[1]]\n","                    \n","            \n","            summ_1 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == 1:\n","                    summ_1 += yhat_yz[tmp_yz]\n","            avg_1 = summ_1/8\n","            summ_2 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == -1:\n","                    summ_2 += yhat_yz[tmp_yz]\n","            avg_2 = summ_2/8\n","            std_1 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == 1:\n","                    std_1 += abs(yhat_yz[tmp_yz]-avg_1)\n","            std_1 /= 8\n","            std_2 = 0\n","            for tmp_yz in self.yz_tuple:\n","                if tmp_yz[0] == -1:\n","                    std_2 += abs(yhat_yz[tmp_yz]-avg_2)\n","            std_2 /= 8\n","#             y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n","#             y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            \n","            if std_1 > std_2:\n","                if yhat_yz[(1.0, (0, 0, 0))] > avg_1:\n","                    self.lb1_1 += self.alpha\n","                    if self.lb1_1 > 1:\n","                        self.lb1_1 = 1\n","                else:\n","                    self.lb1_1 -= self.alpha\n","                    if self.lb1_1 < 0:\n","                        self.lb1_1 = 0\n","\n","                if yhat_yz[(1.0, (0, 0, 1))] > avg_1:\n","                    self.lb1_2 += self.alpha\n","                    if self.lb1_2 > 1:\n","                        self.lb1_2 = 1\n","                else:\n","                    self.lb1_2 -= self.alpha\n","                    if self.lb1_2 < 0:\n","                        self.lb1_2 = 0\n","\n","                if yhat_yz[(1.0, (0, 1, 0))] > avg_1:\n","                    self.lb1_3 += self.alpha\n","                    if self.lb1_3 > 1:\n","                        self.lb1_3 = 1\n","                else:\n","                    self.lb1_3 -= self.alpha\n","                    if self.lb1_3 < 0:\n","                        self.lb1_3 = 0\n","\n","                if yhat_yz[(1.0, (0, 1, 1))] > avg_1:\n","                    self.lb1_4 += self.alpha\n","                    if self.lb1_4 > 1:\n","                        self.lb1_4 = 1\n","                else:\n","                    self.lb1_4 -= self.alpha\n","                    if self.lb1_4 < 0:\n","                        self.lb1_4 = 0\n","\n","                if yhat_yz[(1.0, (1, 0, 0))] > avg_1:\n","                    self.lb1_5 += self.alpha\n","                    if self.lb1_5 > 1:\n","                        self.lb1_5 = 1\n","                else:\n","                    self.lb1_5 -= self.alpha\n","                    if self.lb1_5 < 0:\n","                        self.lb1_5 = 0\n","\n","                if yhat_yz[(1.0, (1, 0, 1))] > avg_1:\n","                    self.lb1_6 += self.alpha\n","                    if self.lb1_6 > 1:\n","                        self.lb1_6 = 1\n","                else:\n","                    self.lb1_6 -= self.alpha\n","                    if self.lb1_6 < 0:\n","                        self.lb1_6 = 0\n","\n","                if yhat_yz[(1.0, (1, 1, 0))] > avg_1:\n","                    self.lb1_7 += self.alpha\n","                    if self.lb1_7 > 1:\n","                        self.lb1_7 = 1\n","                else:\n","                    self.lb1_7 -= self.alpha\n","                    if self.lb1_7 < 0:\n","                        self.lb1_7 = 0\n","                    # if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","                    #     self.lb1 += self.alpha\n","                    # else:\n","                    #     self.lb1 -= self.alpha\n","            else:\n","                if yhat_yz[(-1.0, (0, 0, 0))] > avg_2:\n","                    self.lb2_1 += self.alpha\n","                    if self.lb2_1 > 1:\n","                        self.lb2_1 = 1\n","                else:\n","                    self.lb2_1 -= self.alpha\n","                    if self.lb2_1 < 0:\n","                        self.lb2_1 = 0\n","\n","                if yhat_yz[(-1.0, (0, 0, 1))] > avg_2:\n","                    self.lb2_2 += self.alpha\n","                    if self.lb2_2 > 1:\n","                        self.lb2_2 = 1\n","                else:\n","                    self.lb2_2 -= self.alpha\n","                    if self.lb2_2 < 0:\n","                        self.lb2_2 = 0\n","\n","                if yhat_yz[(-1.0, (0, 1, 0))] > avg_2:\n","                    self.lb2_3 += self.alpha\n","                    if self.lb2_3 > 1:\n","                        self.lb2_3 = 1\n","                else:\n","                    self.lb2_3 -= self.alpha\n","                    if self.lb2_3 < 0:\n","                        self.lb2_3 = 0\n","\n","                if yhat_yz[(-1.0, (0, 1, 1))] > avg_2:\n","                    self.lb2_4 += self.alpha\n","                    if self.lb2_4 > 1:\n","                        self.lb2_4 = 1\n","                else:\n","                    self.lb2_4 -= self.alpha\n","                    if self.lb2_4 < 0:\n","                        self.lb2_4 = 0\n","\n","                if yhat_yz[(-1.0, (1, 0, 0))] > avg_2:\n","                    self.lb2_5 += self.alpha\n","                    if self.lb2_5 > 1:\n","                        self.lb2_5 = 1\n","                else:\n","                    self.lb2_5 -= self.alpha\n","                    if self.lb2_5 < 0:\n","                        self.lb2_5 = 0\n","\n","                if yhat_yz[(-1.0, (1, 0, 1))] > avg_2:\n","                    self.lb2_6 += self.alpha\n","                    if self.lb2_6 > 1:\n","                        self.lb2_6 = 1\n","                else:\n","                    self.lb2_6 -= self.alpha\n","                    if self.lb2_6 < 0:\n","                        self.lb2_6 = 0\n","\n","                if yhat_yz[(-1.0, (1, 1, 0))] > avg_2:\n","                    self.lb2_7 += self.alpha\n","                    if self.lb2_7 > 1:\n","                        self.lb2_7 = 1\n","                else:\n","                    self.lb2_7 -= self.alpha\n","                    if self.lb2_7 < 0:\n","                        self.lb2_7 = 0\n","#                     if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n","#                         self.lb1 += self.alpha\n","#                     else:\n","#                         self.lb1 -= self.alpha\n","                    \n","                \n","            if self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 > 1:\n","                minus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 1)/7\n","                self.lb1_1 -= minus\n","                self.lb1_2 -= minus\n","                self.lb1_3 -= minus\n","                self.lb1_4 -= minus\n","                self.lb1_5 -= minus\n","                self.lb1_6 -= minus\n","                self.lb1_7 -= minus\n","            \n","            elif self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 < 0:\n","                plus = (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7 - 0)/7\n","                self.lb1_1 -= plus\n","                self.lb1_2 -= plus\n","                self.lb1_3 -= plus\n","                self.lb1_4 -= plus\n","                self.lb1_5 -= plus\n","                self.lb1_6 -= plus\n","                self.lb1_7 -= plus\n","            \n","            if self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 > 1:\n","                \n","                minus = (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 - 1)/7\n","                self.lb2_1 -= minus\n","                self.lb2_2 -= minus\n","                self.lb2_3 -= minus\n","                self.lb2_4 -= minus\n","                self.lb2_5 -= minus\n","                self.lb2_6 -= minus\n","                self.lb2_7 -= minus\n","            \n","            elif self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 < 0:\n","                plus = (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7 - 0)/7\n","                self.lb2_1 -= plus\n","                self.lb2_2 -= plus\n","                self.lb2_3 -= plus\n","                self.lb2_4 -= plus\n","                self.lb2_5 -= plus\n","                self.lb2_6 -= plus\n","                self.lb2_7 -= plus\n","\n","    def select_fair_robust_sample(self):\n","        \"\"\"Selects fair and robust samples and adjusts the lambda values for fairness. \n","        See our paper for algorithm details.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        logit = self.get_logit()\n","        \n","        self.adjust_lambda(logit)\n","\n","        criterion = torch.nn.BCELoss(reduction = 'none')\n","        \n","        loss = criterion ((F.tanh(logit.squeeze())+1)/2, (self.y_data.squeeze()+1)/2)\n","        profit = torch.max(loss)-loss\n","        \n","        current_weight_sum = {}\n","        \n","        lb_ratio = {}\n","        for tmp_yz in self.yz_tuple:\n","            if tmp_yz == (1.0, (0, 0, 0)):\n","                \n","                lb_ratio[tmp_yz] = self.lb1_1                \n","            elif tmp_yz == (1.0, (0, 0, 1)): \n","                            \n","                lb_ratio[tmp_yz] = self.lb1_2\n","            elif tmp_yz == (1.0, (0, 1, 0)): \n","                lb_ratio[tmp_yz] = self.lb1_3\n","            elif tmp_yz == (1.0, (0, 1, 1)): \n","                lb_ratio[tmp_yz] = self.lb1_4\n","            elif tmp_yz == (1.0, (1, 0, 0)):\n","                lb_ratio[tmp_yz] = self.lb1_5\n","            elif tmp_yz == (1.0, (1, 0, 1)):\n","                            \n","                lb_ratio[tmp_yz] = self.lb1_6\n","            elif tmp_yz == (1.0, (1, 1, 0)):\n","                            \n","                lb_ratio[tmp_yz] = self.lb1_7\n","            elif tmp_yz == (1.0, (1, 1, 1)):\n","                lb_ratio[tmp_yz] = 1 - (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7)\n","            elif tmp_yz == (-1.0, (0, 0, 0)):\n","                            \n","                lb_ratio[tmp_yz] = self.lb2_1\n","            elif tmp_yz == (-1.0, (0, 0, 1)): \n","                lb_ratio[tmp_yz] = self.lb2_2\n","            elif tmp_yz == (-1.0, (0, 1, 0)): \n","                lb_ratio[tmp_yz] = self.lb2_3\n","            elif tmp_yz == (-1.0, (0, 1, 1)): \n","                lb_ratio[tmp_yz] = self.lb2_4\n","            elif tmp_yz == (-1.0, (1, 0, 0)): \n","                lb_ratio[tmp_yz] = self.lb2_5\n","            elif tmp_yz == (-1.0, (1, 0, 1)): \n","                lb_ratio[tmp_yz] = self.lb2_6\n","            elif tmp_yz == (-1.0, (1, 1, 0)): \n","                lb_ratio[tmp_yz] = self.lb2_7\n","            elif tmp_yz == (-1.0, (1, 1, 1)):\n","                            \n","                lb_ratio[tmp_yz] = 1 - (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb2_5 + self.lb2_6 + self.lb2_7)\n","                            \n","            current_weight_sum[tmp_yz] = 0\n","        (_, sorted_index) = torch.topk(profit, len(profit), largest=True, sorted=True)\n","        \n","        clean_index = []\n","        \n","        total_selected = 0\n","        \n","        desired_size = int(self.tau * len(self.y_data))\n","        \n","        for j in sorted_index:\n","            tmp_y = self.y_data[j].item()\n","            tmp_z = tuple((self.z1_data[j].item(), self.z2_data[j].item(), self.z3_data[j].item()))\n","            current_weight_list = list(current_weight_sum.values())\n","            \n","            if total_selected >= desired_size:\n","                break\n","            if all(i < desired_size for i in current_weight_list):\n","                clean_index.append(j)\n","                for tmp_yz in self.yz_tuple:\n","                    if tmp_yz[0] != tmp_y:\n","                        current_weight_sum[tmp_yz] += 1\n","                    elif tmp_yz[1] != tmp_z:\n","                        current_weight_sum[tmp_yz] += 1 - lb_ratio[tmp_yz]\n","                    else:\n","                        current_weight_sum[tmp_yz] += 2 - lb_ratio[tmp_yz]\n","                            \n","                            \n","                total_selected += 1        \n","        \n","        clean_index = torch.LongTensor(clean_index).cuda()\n","        \n","        self.batch_num = int(len(clean_index)/self.batch_size)\n","        \n","        # Update the variables\n","        self.clean_index = clean_index\n","        \n","        for tmp_z in self.z_item:\n","            combined = torch.cat((self.z_index[tmp_z], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_z_index[tmp_z] = intersection \n","            \n","        for tmp_y in self.y_item:\n","            combined = torch.cat((self.y_index[tmp_y], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_y_index[tmp_y] = intersection\n","        \n","        for tmp_yz in self.yz_tuple:\n","            combined = torch.cat((self.yz_index[tmp_yz], self.clean_index))\n","            uniques, counts = combined.unique(return_counts=True)\n","            intersection = uniques[counts > 1]\n","            \n","            self.clean_yz_index[tmp_yz] = intersection\n","        \n","        \n","        for tmp_z in self.z_item:\n","            self.clean_z_len[tmp_z] = len(self.clean_z_index[tmp_z])\n","            \n","        for tmp_y in self.y_item:\n","            self.clean_y_len[tmp_y] = len(self.clean_y_index[tmp_y])\n","        \n","        for tmp_yz in self.yz_tuple:\n","            self.clean_yz_len[tmp_yz] = len(self.clean_yz_index[tmp_yz])\n","            \n","        \n","        return clean_index\n","        \n","    \n","    def select_batch_replacement(self, batch_size, full_index, batch_num, replacement = False, weight = None):\n","        \"\"\"Selects a certain number of batches based on the given batch size.\n","        \n","        Args: \n","            batch_size: An integer for the data size in a batch.\n","            full_index: An array containing the candidate data indices.\n","            batch_num: An integer indicating the number of batches.\n","            replacement: A boolean indicating whether a batch consists of data with or without replacement.\n","        \n","        Returns:\n","            Indexes that indicate the data.\n","            \n","        \"\"\"\n","        \n","        select_index = []\n","        \n","        if replacement == True:\n","            for _ in range(batch_num):\n","                if weight == None:\n","                    weight_norm = weight/torch.sum(weight)\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False, p = weight_norm))\n","                else:\n","                    select_index.append(np.random.choice(full_index, batch_size, replace = False))\n","        else:\n","            tmp_index = full_index.detach().cpu().numpy().copy()\n","            random.shuffle(tmp_index)\n","            \n","            start_idx = 0\n","            for i in range(batch_num):\n","                if start_idx + batch_size > len(full_index):\n","                    select_index.append(np.concatenate((tmp_index[start_idx:], tmp_index[ : batch_size - (len(full_index)-start_idx)])))\n","                    \n","                    start_idx = len(full_index)-start_idx\n","                else:\n","\n","                    select_index.append(tmp_index[start_idx:start_idx + batch_size])\n","                    start_idx += batch_size\n","            \n","        return select_index\n","\n","\n","    \n","    def decide_fair_batch_size(self):\n","        \"\"\"Calculates each class size based on the lambda values (lb1 and lb2) for fairness.\n","        \n","        Returns:\n","            Each class size for fairness.\n","            \n","        \"\"\"\n","        \n","        each_size = {}\n","        summ_1 = 0\n","        summ_2 = 0\n","\n","        for tmp_yz in self.yz_tuple:\n","            self.S[tmp_yz] = self.batch_size * (self.clean_yz_len[tmp_yz])/len(self.clean_index)\n","            if tmp_yz[0] == 1:\n","                summ_1 += self.S[tmp_yz]\n","            else:\n","                summ_2 += self.S[tmp_yz]\n","\n","        # Based on the updated lambdas, determine the size of each class in a batch\n","        if self.fairness_type == 'eqopp':\n","            each_size[(1.0, (0, 0, 0))] = round(self.lb1_1 * summ_1)\n","            each_size[(1.0, (0, 0, 1))] = round(self.lb1_2 * summ_1)\n","            each_size[(1.0, (0, 1, 0))] = round(self.lb1_3 * summ_1)\n","            each_size[(1.0, (0, 1, 1))] = round(self.lb1_4 * summ_1)\n","            each_size[(1.0, (1, 0, 0))] = round(self.lb1_5 * summ_1)\n","            each_size[(1.0, (1, 0, 1))] = round(self.lb1_6 * summ_1)\n","            each_size[(1.0, (1, 1, 0))] = round(self.lb1_7 * summ_1)\n","            each_size[(1.0, (1, 1, 1))] = round((1 - (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7))*summ_1)\n","            each_size[(-1.0, (0, 0, 0))] = round(self.S[(-1.0, (0, 0, 0))])\n","            each_size[(-1.0, (0, 0, 1))] = round(self.S[(-1.0, (0, 0, 1))])\n","            each_size[(-1.0, (0, 1, 0))] = round(self.S[(-1.0, (0, 1, 0))])\n","            each_size[(-1.0, (0, 1, 1))] = round(self.S[(-1.0, (0, 1, 1))])\n","            each_size[(-1.0, (1, 0, 0))] = round(self.S[(-1.0, (1, 0, 0))])\n","            each_size[(-1.0, (1, 0, 1))] = round(self.S[(-1.0, (1, 0, 1))])\n","            each_size[(-1.0, (1, 1, 0))] = round(self.S[(-1.0, (1, 1, 0))])\n","            each_size[(-1.0, (1, 1, 1))] = round(self.S[(-1.0, (1, 1, 1))])\n","#             each_size[(-1,1)] = round(self.S[(-1,1)])\n","#             each_size[(-1,0)] = round(self.S[(-1,0)])\n","                            \n","            \n","\n","        elif self.fairness_type == 'eqodds':\n","\n","            each_size[(1.0, (0, 0, 0))] = round(self.lb1_1 * summ_1)\n","            each_size[(1.0, (0, 0, 1))] = round(self.lb1_2 * summ_1)\n","            each_size[(1.0, (0, 1, 0))] = round(self.lb1_3 * summ_1)\n","            each_size[(1.0, (0, 1, 1))] = round(self.lb1_4 * summ_1)\n","            each_size[(1.0, (1, 0, 0))] = round(self.lb1_5 * summ_1)\n","            each_size[(1.0, (1, 0, 1))] = round(self.lb1_6 * summ_1)\n","            each_size[(1.0, (1, 1, 0))] = round(self.lb1_7 * summ_1)\n","            each_size[(1.0, (1, 1, 1))] = round((1 - (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7))*summ_1)\n","            each_size[(-1.0, (0, 0, 0))] = round(self.lb2_1 * summ_2)\n","            each_size[(-1.0, (0, 0, 1))] = round(self.lb2_2 * summ_2)\n","            each_size[(-1.0, (0, 1, 0))] = round(self.lb2_3 * summ_2)\n","            each_size[(-1.0, (0, 1, 1))] = round(self.lb2_4 * summ_2)\n","            each_size[(-1.0, (1, 0, 0))] = round(self.lb2_5 * summ_2)\n","            each_size[(-1.0, (1, 0, 1))] = round(self.lb2_6 * summ_2)\n","            each_size[(-1.0, (1, 1, 0))] = round(self.lb2_7 * summ_2)\n","            each_size[(-1.0, (1, 1, 1))] = round((1 - (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb1_5 + self.lb2_6 + self.lb1_7))*summ_2)\n","        elif self.fairness_type == 'dp':\n","\n","            \n","            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n","            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n","            each_size[(1.0, (0, 0, 0))] = round(self.lb1_1 * summ_1)\n","            each_size[(1.0, (0, 0, 1))] = round(self.lb1_2 * summ_1)\n","            each_size[(1.0, (0, 1, 0))] = round(self.lb1_3 * summ_1)\n","            each_size[(1.0, (0, 1, 1))] = round(self.lb1_4 * summ_1)\n","            each_size[(1.0, (1, 0, 0))] = round(self.lb1_5 * summ_1)\n","            each_size[(1.0, (1, 0, 1))] = round(self.lb1_6 * summ_1)\n","            each_size[(1.0, (1, 1, 0))] = round(self.lb1_7 * summ_1)\n","            each_size[(1.0, (1, 1, 1))] = round((1 - (self.lb1_1 + self.lb1_2 + self.lb1_3 + self.lb1_4 + self.lb1_5 + self.lb1_6 + self.lb1_7))*summ_1)\n","            each_size[(-1.0, (0, 0, 0))] = round(self.lb2_1 * summ_2)\n","            each_size[(-1.0, (0, 0, 1))] = round(self.lb2_2 * summ_2)\n","            each_size[(-1.0, (0, 1, 0))] = round(self.lb2_3 * summ_2)\n","            each_size[(-1.0, (0, 1, 1))] = round(self.lb2_4 * summ_2)\n","            each_size[(-1.0, (1, 0, 0))] = round(self.lb2_5 * summ_2)\n","            each_size[(-1.0, (1, 0, 1))] = round(self.lb2_6 * summ_2)\n","            each_size[(-1.0, (1, 1, 0))] = round(self.lb2_7 * summ_2)\n","            each_size[(-1.0, (1, 1, 1))] = round((1 - (self.lb2_1 + self.lb2_2 + self.lb2_3 + self.lb2_4 + self.lb1_5 + self.lb2_6 + self.lb1_7))*summ_2)        \n","        return each_size\n","        \n","        \n","    \n","    def __iter__(self):\n","        \"\"\"Iters the full process of fair and robust sample selection for serving the batches to training.\n","        \n","        Returns:\n","            Indexes that indicate the data in each batch.\n","            \n","        \"\"\"\n","        self.count_epoch += 1\n","        \n","        if self.count_epoch > self.warm_start:\n","\n","            _ = self.select_fair_robust_sample()\n","\n","\n","            each_size = self.decide_fair_batch_size()\n","            sort_index_y_1_z_000 = self.select_batch_replacement(each_size[(1.0, (0, 0, 0))], self.clean_yz_index[(1.0, (0, 0, 0))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_001 = self.select_batch_replacement(each_size[(1.0, (0, 0, 1))], self.clean_yz_index[(1.0, (0, 0, 1))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_010 = self.select_batch_replacement(each_size[(1.0, (0, 1, 0))], self.clean_yz_index[(1.0, (0, 1, 0))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_011 = self.select_batch_replacement(each_size[(1.0, (0, 1, 1))], self.clean_yz_index[(1.0, (0, 1, 1))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_100 = self.select_batch_replacement(each_size[(1.0, (1, 0, 0))], self.clean_yz_index[(1.0, (1, 0, 0))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_101 = self.select_batch_replacement(each_size[(1.0, (1, 0, 1))], self.clean_yz_index[(1.0, (1, 0, 1))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_110 = self.select_batch_replacement(each_size[(1.0, (1, 1, 0))], self.clean_yz_index[(1.0, (1, 1, 0))], self.batch_num, self.replacement)\n","            sort_index_y_1_z_111 = self.select_batch_replacement(each_size[(1.0, (1, 1, 1))], self.clean_yz_index[(1.0, (1, 1, 1))], self.batch_num, self.replacement)\n","                            \n","            sort_index_y_0_z_000 = self.select_batch_replacement(each_size[(-1.0, (0, 0, 0))], self.clean_yz_index[(-1.0, (0, 0, 0))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_001 = self.select_batch_replacement(each_size[(-1.0, (0, 0, 1))], self.clean_yz_index[(-1.0, (0, 0, 1))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_010 = self.select_batch_replacement(each_size[(-1.0, (0, 1, 0))], self.clean_yz_index[(-1.0, (0, 1, 0))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_011 = self.select_batch_replacement(each_size[(-1.0, (0, 1, 1))], self.clean_yz_index[(-1.0, (0, 1, 1))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_100 = self.select_batch_replacement(each_size[(-1.0, (1, 0, 0))], self.clean_yz_index[(-1.0, (1, 0, 0))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_101 = self.select_batch_replacement(each_size[(-1.0, (1, 0, 1))], self.clean_yz_index[(-1.0, (1, 0, 1))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_110 = self.select_batch_replacement(each_size[(-1.0, (1, 1, 0))], self.clean_yz_index[(-1.0, (1, 1, 0))], self.batch_num, self.replacement)\n","            sort_index_y_0_z_111 = self.select_batch_replacement(each_size[(-1.0, (1, 1, 1))], self.clean_yz_index[(-1.0, (1, 1, 1))], self.batch_num, self.replacement)\n","                            \n","#             sort_index_y_0_z_1 = self.select_batch_replacement(each_size[(-1, 1)], self.clean_yz_index[(-1,1)], self.batch_num, self.replacement)\n","#             sort_index_y_1_z_0 = self.select_batch_replacement(each_size[(1, 0)], self.clean_yz_index[(1,0)], self.batch_num, self.replacement)\n","#             sort_index_y_0_z_0 = self.select_batch_replacement(each_size[(-1, 0)], self.clean_yz_index[(-1,0)], self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                key_in_fairbatch = sort_index_y_0_z_000[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_000[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_001[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_001[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_010[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_010[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_011[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_011[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_100[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_100[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_101[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_101[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_110[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_110[i].copy()))\n","                key_in_fairbatch = sort_index_y_0_z_111[i].copy()\n","                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_111[i].copy()))\n","                            \n","#                 key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_0_z_1[i].copy()))\n","#                 key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_1[i].copy()))\n","\n","                random.shuffle(key_in_fairbatch)\n","\n","                yield key_in_fairbatch\n","\n","        else:\n","            entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n","\n","            sort_index = self.select_batch_replacement(self.batch_size, entire_index, self.batch_num, self.replacement)\n","\n","            for i in range(self.batch_num):\n","                yield sort_index[i]\n","        \n","                               \n","\n","    def __len__(self):\n","        \"\"\"Returns the length of data.\"\"\"\n","        \n","        return len(self.y_data)"],"metadata":{"id":"tgkI3og_N0Fb","executionInfo":{"status":"ok","timestamp":1643937340208,"user_tz":-540,"elapsed":52070,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oEum8UUyOSPN","executionInfo":{"status":"ok","timestamp":1643937340209,"user_tz":-540,"elapsed":31,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_v6AWHDmLzFc","executionInfo":{"status":"ok","timestamp":1643937340210,"user_tz":-540,"elapsed":9,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["import sys, os\n","import numpy as np\n","import math\n","import random\n","import itertools\n","import copy\n","# from google.colab import files\n","# src = list(files.upload().values())[0]\n","# open('models.py','wb').write(src)\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import Sampler\n","import torch\n","# from models import LogisticRegression, weights_init_normal, test_model\n","# open('FairRobustSampler.py','wb').write(src)\n","# from FairRobustSampler import FairRobust, CustomDataset\n","\n","from argparse import Namespace\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"e3dA1PkBMHCp","executionInfo":{"status":"ok","timestamp":1643937340210,"user_tz":-540,"elapsed":7,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7c1R4r1SLzFf"},"source":["## Load and process the data\n","In the synthetic_data directory, there are a total of 11 numpy files including training data (both clean and noisy), validation data, and test data. Note that the validation data is utilized for another method in the paper (i.e., FR-Train), so the data is not used in this program."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8U55DSmHLzFh","executionInfo":{"status":"ok","timestamp":1643937345965,"user_tz":-540,"elapsed":5762,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["xz_train = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/X_train.npy')\n","y_train = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/y_train.npy')\n","z1_train = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z1_train.npy')\n","z2_train = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z2_train.npy')\n","z3_train = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z3_train.npy')\n","\n","yz123_noise = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/y_noisy_z123.npy')\n","poi_ratio = 0.2\n","xz_test = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/X_test.npy')\n","y_test = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/y_test.npy') \n","z1_test = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z1_test.npy')\n","z2_test = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z2_test.npy')\n","z3_test = np.load('/content/drive/MyDrive/fair-robust-selection/data_new/z3_test.npy')\n","\n","\n","xz_train = torch.FloatTensor(xz_train)\n","y_train = torch.FloatTensor(y_train)\n","z1_train = torch.FloatTensor(z1_train)\n","z2_train = torch.FloatTensor(z2_train)\n","z3_train = torch.FloatTensor(z3_train)\n","\n","yz123_noise = torch.FloatTensor(yz123_noise)\n","\n","xz_test = torch.FloatTensor(xz_test)\n","y_test = torch.FloatTensor(y_test)\n","z1_test = torch.FloatTensor(z1_test)\n","z2_test = torch.FloatTensor(z2_test)\n","z3_test = torch.FloatTensor(z3_test)"]},{"cell_type":"code","source":["xz_train.shape"],"metadata":{"id":"M8GVsd7FG-s5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643937345967,"user_tz":-540,"elapsed":28,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}},"outputId":"72ecd0d3-6110-4c79-f347-ad37607d2793"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([21815, 45])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ca3md6K7LzFk","executionInfo":{"status":"ok","timestamp":1643937345968,"user_tz":-540,"elapsed":24,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["train_data = CustomDataset1(xz_train, yz123_noise, z1_train, z2_train, z3_train)\n","model = LogisticRegression(45,1)\n","parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bKUuxQbQLzFm","executionInfo":{"status":"ok","timestamp":1643937345969,"user_tz":-540,"elapsed":24,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["f = FairRobust2(model, train_data.x, train_data.y, train_data.z1, train_data.z2, train_data.z3, target_fairness = 'eqodds', parameters = parameters, replacement = False, seed = 0)"]},{"cell_type":"code","source":[""],"metadata":{"id":"4piNKHz7N-9b","executionInfo":{"status":"ok","timestamp":1643937345969,"user_tz":-540,"elapsed":23,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"F1tQ3AyELzFr","executionInfo":{"status":"ok","timestamp":1643937345970,"user_tz":-540,"elapsed":24,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["if torch.cuda.is_available():\n","    xz_train = xz_train.cuda()\n","    yz123_noise = yz123_noise.cuda()\n","    y_train = y_train.cuda()\n","    z1_train = z1_train.cuda()\n","    z2_train = z2_train.cuda()\n","    z3_train = z3_train.cuda()\n","    \n","    xz_test = xz_test.cuda()\n","    y_test = y_test.cuda()\n","    z1_test = z1_test.cuda()\n","    z2_test = z2_test.cuda()\n","    z3_test = z3_test.cuda()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Pfrt60lULzFt","executionInfo":{"status":"ok","timestamp":1643937345970,"user_tz":-540,"elapsed":23,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c48070f-9155-4717-eb12-9ff8301fed4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["---------- Number of Data ----------\n","Train data : 21815, Test data : 10746 \n","------------------------------------\n"]}],"source":["print(\"---------- Number of Data ----------\" )\n","print(\n","    \"Train data : %d, Test data : %d \"\n","    % (len(y_train), len(y_test))\n",")       \n","print(\"------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"eUeSCM6JLzFv"},"source":["## Training function"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"N821CzKTLzFx","executionInfo":{"status":"ok","timestamp":1643937345971,"user_tz":-540,"elapsed":16,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["def run_epoch(model, train_features, labels, optimizer, criterion):\n","    \"\"\"Trains the model with the given train data.\n","\n","    Args:\n","        model: A torch model to train.\n","        train_features: A torch tensor indicating the train features.\n","        labels: A torch tensor indicating the true labels.\n","        optimizer: A torch optimizer.\n","        criterion: A torch criterion.\n","\n","    Returns:\n","        loss values.\n","    \"\"\"\n","    \n","    optimizer.zero_grad()\n","\n","    label_predicted = model.forward(train_features)\n","    loss  = criterion((F.tanh(label_predicted.squeeze())+1)/2, (labels.squeeze()+1)/2)\n","    loss.backward()\n","\n","    optimizer.step()\n","    \n","    return loss.item()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"akOXUuL4LzFy","executionInfo":{"status":"ok","timestamp":1643937345971,"user_tz":-540,"elapsed":15,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"Gcn0QdSrLzF0"},"source":["# 1. Fair and Robust Sample Selection w.r.t. Equalized Odds\n","### The results are in the experiments of the paper."]},{"cell_type":"code","execution_count":12,"metadata":{"scrolled":true,"id":"b7De2kf3LzF1","executionInfo":{"status":"error","timestamp":1643937346660,"user_tz":-540,"elapsed":702,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}},"colab":{"base_uri":"https://localhost:8080/","height":398},"outputId":"1f96265b-d1f4-4944-9105-7e0cf83c8efa"},"outputs":[{"output_type":"stream","name":"stdout","text":["< Seed: 0 >\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ad2500d57c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     model = LogisticRegression(3,1).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["full_tests_z1 = []\n","full_tests_z2 = []\n","full_tests_z3 = []\n","\n","parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n","\n","# Set the train data\n","train_data = CustomDataset1(xz_train, yz123_noise, z1_train, z2_train, z3_train)\n","\n","seeds = [0]\n","\n","for seed in seeds:\n","    \n","    print(\"< Seed: {} >\".format(seed))\n","    \n","    # ---------------------\n","    #  Initialize model, optimizer, and criterion\n","    # ---------------------\n","    \n","#     model = LogisticRegression(3,1).cuda()\n","    model = LogisticRegression(45,1)\n","    model.cuda()\n","    torch.manual_seed(seed)\n","    model.apply(weights_init_normal)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n","    criterion = torch.nn.BCELoss()\n","\n","    losses = []\n","    \n","    # ---------------------\n","    #  Define FairRobust and DataLoader\n","    # ---------------------\n","\n","    sampler = FairRobust1(model, train_data.x, train_data.y, train_data.z1, train_data.z2, train_data.z3, target_fairness = 'eqodds', parameters = parameters, replacement = False, seed = seed)\n","    train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n","\n","    # ---------------------\n","    #  Model training\n","    # ---------------------\n","    for epoch in range(450):\n","        print(epoch, end=\"\\r\")\n","        \n","        tmp_loss = []\n","        \n","        for batch_idx, (data, target, z1, z2, z3) in enumerate (train_loader):\n","            loss = run_epoch (model, data, target, optimizer, criterion)\n","            tmp_loss.append(loss)\n","            \n","        losses.append(sum(tmp_loss)/len(tmp_loss))\n","        \n","\n","    tmp_test_z1 = test_model(model, xz_test, y_test, z1_test)\n","    tmp_test_z2 = test_model(model, xz_test, y_test, z2_test)\n","    tmp_test_z3 = test_model(model, xz_test, y_test, z3_test)\n","    full_tests_z1.append(tmp_test_z1)\n","    full_tests_z2.append(tmp_test_z2)\n","    full_tests_z3.append(tmp_test_z3)\n","    \n","    print(\"z1  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z1['Acc'], tmp_test_z1['EqOdds_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"z2  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z2['EqOdds_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"z3  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z3['Acc'], tmp_test_z3['EqOdds_diff']))\n","    print(\"----------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4bzI9_0LzF3","executionInfo":{"status":"aborted","timestamp":1643937346655,"user_tz":-540,"elapsed":20,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["tmp_acc = []\n","tmp_dp_z1 = []\n","tmp_eo_z1 = []\n","tmp_dp_z2 = []\n","tmp_eo_z2 = []\n","tmp_dp_z3 = []\n","tmp_eo_z3 = []\n","for i in range(len(seeds)):\n","    tmp_acc.append(full_tests_z1[i]['Acc'])\n","print(\"Test accuracy (avg): {}\".format(sum(tmp_acc)/len(tmp_acc)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z1.append(full_tests_z1[i]['EqOdds_diff'])\n","    tmp_dp_z1.append(full_tests_z1[i]['DP_diff'])\n","print(\"z1 EO disparity  (avg): {}\".format(sum(tmp_eo_z1)/len(tmp_eo_z1)))\n","print(\"z1 DP disparity  (avg): {}\".format(sum(tmp_dp_z1)/len(tmp_dp_z1)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z2.append(full_tests_z2[i]['EqOdds_diff'])\n","    tmp_dp_z2.append(full_tests_z2[i]['DP_diff'])\n","print(\"z2 EO disparity  (avg): {}\".format(sum(tmp_eo_z2)/len(tmp_eo_z2)))\n","print(\"z2 DP disparity  (avg): {}\".format(sum(tmp_dp_z2)/len(tmp_dp_z2)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z3.append(full_tests_z3[i]['EqOdds_diff'])\n","    tmp_dp_z3.append(full_tests_z3[i]['DP_diff'])\n","print(\"z3 EO disparity  (avg): {}\".format(sum(tmp_eo_z3)/len(tmp_eo_z3)))\n","print(\"z3 DP disparity  (avg): {}\".format(sum(tmp_dp_z3)/len(tmp_dp_z3)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"PAEgFoTbOi3O","executionInfo":{"status":"aborted","timestamp":1643937346656,"user_tz":-540,"elapsed":21,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["# full_tests_z1 = []\n","# full_tests_z2 = []\n","# full_tests_z3 = []\n","\n","# parameters = Namespace(warm_start=450, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n","\n","# # Set the train data\n","# train_data = CustomDataset1(xz_train, yz123_noise, z1_train, z2_train, z3_train)\n","\n","# seeds = [0]\n","\n","# for seed in seeds:\n","    \n","#     print(\"< Seed: {} >\".format(seed))\n","    \n","#     # ---------------------\n","#     #  Initialize model, optimizer, and criterion\n","#     # ---------------------\n","    \n","# #     model = LogisticRegression(3,1).cuda()\n","#     model = LogisticRegression(45,1)\n","#     model.cuda()\n","#     torch.manual_seed(seed)\n","#     model.apply(weights_init_normal)\n","\n","#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n","#     criterion = torch.nn.BCELoss()\n","\n","#     losses = []\n","    \n","#     # ---------------------\n","#     #  Define FairRobust and DataLoader\n","#     # ---------------------\n","\n","#     sampler = FairRobust(model, train_data.x, train_data.y, train_data.z1, train_data.z2, train_data.z3, target_fairness = 'eqodds', parameters = parameters, replacement = False, seed = seed)\n","#     train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n","\n","#     # ---------------------\n","#     #  Model training\n","#     # ---------------------\n","#     for epoch in range(450):\n","#         print(epoch, end=\"\\r\")\n","        \n","#         tmp_loss = []\n","        \n","#         for batch_idx, (data, target, z1, z2, z3) in enumerate (train_loader):\n","#             loss = run_epoch (model, data, target, optimizer, criterion)\n","#             tmp_loss.append(loss)\n","            \n","#         losses.append(sum(tmp_loss)/len(tmp_loss))\n","        \n","\n","#     tmp_test_z1 = test_model(model, xz_test, y_test, z1_test)\n","#     tmp_test_z2 = test_model(model, xz_test, y_test, z2_test)\n","#     tmp_test_z3 = test_model(model, xz_test, y_test, z3_test)\n","#     full_tests_z1.append(tmp_test_z1)\n","#     full_tests_z2.append(tmp_test_z2)\n","#     full_tests_z3.append(tmp_test_z3)\n","    \n","#     print(\"z1  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z1['Acc'], tmp_test_z1['EqOdds_diff']))\n","#     print(\"----------------------------------------------------------------------\")\n","#     print(\"z2  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z2['EqOdds_diff']))\n","#     print(\"----------------------------------------------------------------------\")\n","#     print(\"z3  Test accuracy: {}, EO disparity: {}\".format(tmp_test_z3['Acc'], tmp_test_z3['EqOdds_diff']))\n","#     print(\"----------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ttLSdU8Ox-s","executionInfo":{"status":"aborted","timestamp":1643937346657,"user_tz":-540,"elapsed":21,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["# tmp_acc = []\n","# tmp_dp_z1 = []\n","# tmp_eo_z1 = []\n","# tmp_dp_z2 = []\n","# tmp_eo_z2 = []\n","# tmp_dp_z3 = []\n","# tmp_eo_z3 = []\n","# for i in range(len(seeds)):\n","#     tmp_acc.append(full_tests_z1[i]['Acc'])\n","# print(\"Test accuracy (avg): {}\".format(sum(tmp_acc)/len(tmp_acc)))\n","\n","# for i in range(len(seeds)):\n","#     tmp_eo_z1.append(full_tests_z1[i]['EqOdds_diff'])\n","#     tmp_dp_z1.append(full_tests_z1[i]['DP_diff'])\n","# print(\"z1 EO disparity  (avg): {}\".format(sum(tmp_eo_z1)/len(tmp_eo_z1)))\n","# print(\"z1 DP disparity  (avg): {}\".format(sum(tmp_dp_z1)/len(tmp_dp_z1)))\n","\n","# for i in range(len(seeds)):\n","#     tmp_eo_z2.append(full_tests_z2[i]['EqOdds_diff'])\n","#     tmp_dp_z2.append(full_tests_z2[i]['DP_diff'])\n","# print(\"z2 EO disparity  (avg): {}\".format(sum(tmp_eo_z2)/len(tmp_eo_z2)))\n","# print(\"z2 DP disparity  (avg): {}\".format(sum(tmp_dp_z2)/len(tmp_dp_z2)))\n","\n","# for i in range(len(seeds)):\n","#     tmp_eo_z3.append(full_tests_z3[i]['EqOdds_diff'])\n","#     tmp_dp_z3.append(full_tests_z3[i]['DP_diff'])\n","# print(\"z3 EO disparity  (avg): {}\".format(sum(tmp_eo_z3)/len(tmp_eo_z3)))\n","# print(\"z3 DP disparity  (avg): {}\".format(sum(tmp_dp_z3)/len(tmp_dp_z3)))\n"]},{"cell_type":"markdown","metadata":{"id":"DgC7lEExLzF7"},"source":["# 2. Fair and Robust Sample Selection w.r.t. Demographic Parity\n","### The results are in the experiments of the paper."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"yjO6TNsaLzF8","executionInfo":{"status":"aborted","timestamp":1643937346657,"user_tz":-540,"elapsed":21,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["full_tests_z1 = []\n","full_tests_z2 = []\n","full_tests_z3 = []\n","\n","parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n","\n","# Set the train data\n","train_data = CustomDataset1(xz_train, yz123_noise, z1_train, z2_train, z3_train)\n","\n","seeds = [0]\n","\n","for seed in seeds:\n","    \n","    print(\"< Seed: {} >\".format(seed))\n","    \n","    # ---------------------\n","    #  Initialize model, optimizer, and criterion\n","    # ---------------------\n","    \n","#     model = LogisticRegression(3,1).cuda()\n","    model = LogisticRegression(45,1)\n","    model.cuda()\n","    torch.manual_seed(seed)\n","    model.apply(weights_init_normal)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n","    criterion = torch.nn.BCELoss()\n","\n","    losses = []\n","    \n","    # ---------------------\n","    #  Define FairRobust and DataLoader\n","    # ---------------------\n","\n","    sampler = FairRobust2(model, train_data.x, train_data.y, train_data.z1, train_data.z2, train_data.z3, target_fairness = 'dp', parameters = parameters, replacement = False, seed = seed)\n","    train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n","\n","    # ---------------------\n","    #  Model training\n","    # ---------------------\n","    for epoch in range(450):\n","        print(epoch, end=\"\\r\")\n","        \n","        tmp_loss = []\n","        \n","        for batch_idx, (data, target, z1, z2, z3) in enumerate (train_loader):\n","            loss = run_epoch (model, data, target, optimizer, criterion)\n","            tmp_loss.append(loss)\n","            \n","        losses.append(sum(tmp_loss)/len(tmp_loss))\n","        \n","\n","    tmp_test_z1 = test_model(model, xz_test, y_test, z1_test)\n","    tmp_test_z2 = test_model(model, xz_test, y_test, z2_test)\n","    tmp_test_z3 = test_model(model, xz_test, y_test, z3_test)\n","    full_tests_z1.append(tmp_test_z1)\n","    full_tests_z2.append(tmp_test_z2)\n","    full_tests_z3.append(tmp_test_z3)\n","    \n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z1['Acc'], tmp_test_z1['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z2['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z3['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ewFc-bLKLzF-","executionInfo":{"status":"aborted","timestamp":1643937346658,"user_tz":-540,"elapsed":22,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["\n","tmp_acc = []\n","tmp_dp_z1 = []\n","tmp_eo_z1 = []\n","tmp_dp_z2 = []\n","tmp_eo_z2 = []\n","tmp_dp_z3 = []\n","tmp_eo_z3 = []\n","for i in range(len(seeds)):\n","    tmp_acc.append(full_tests_z1[i]['Acc'])\n","print(\"Test accuracy (avg): {}\".format(sum(tmp_acc)/len(tmp_acc)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z1.append(full_tests_z1[i]['EqOdds_diff'])\n","    tmp_dp_z1.append(full_tests_z1[i]['DP_diff'])\n","print(\"z1 EO disparity  (avg): {}\".format(sum(tmp_eo_z1)/len(tmp_eo_z1)))\n","print(\"z1 DP disparity  (avg): {}\".format(sum(tmp_dp_z1)/len(tmp_dp_z1)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z2.append(full_tests_z2[i]['EqOdds_diff'])\n","    tmp_dp_z2.append(full_tests_z2[i]['DP_diff'])\n","print(\"z2 EO disparity  (avg): {}\".format(sum(tmp_eo_z2)/len(tmp_eo_z2)))\n","print(\"z2 DP disparity  (avg): {}\".format(sum(tmp_dp_z2)/len(tmp_dp_z2)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z3.append(full_tests_z3[i]['EqOdds_diff'])\n","    tmp_dp_z3.append(full_tests_z3[i]['DP_diff'])\n","print(\"z3 EO disparity  (avg): {}\".format(sum(tmp_eo_z3)/len(tmp_eo_z3)))\n","print(\"z3 DP disparity  (avg): {}\".format(sum(tmp_dp_z3)/len(tmp_dp_z3)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"InNemmXZO9bm","executionInfo":{"status":"aborted","timestamp":1643937346659,"user_tz":-540,"elapsed":23,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["full_tests_z1 = []\n","full_tests_z2 = []\n","full_tests_z3 = []\n","\n","parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n","\n","# Set the train data\n","train_data = CustomDataset1(xz_train, yz123_noise, z1_train, z2_train, z3_train)\n","\n","seeds = [0]\n","\n","for seed in seeds:\n","    \n","    print(\"< Seed: {} >\".format(seed))\n","    \n","    # ---------------------\n","    #  Initialize model, optimizer, and criterion\n","    # ---------------------\n","    \n","#     model = LogisticRegression(3,1).cuda()\n","    model = LogisticRegression(45,1)\n","    model.cuda()\n","    torch.manual_seed(seed)\n","    model.apply(weights_init_normal)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n","    criterion = torch.nn.BCELoss()\n","\n","    losses = []\n","    \n","    # ---------------------\n","    #  Define FairRobust and DataLoader\n","    # ---------------------\n","\n","    sampler = FairRobust1(model, train_data.x, train_data.y, train_data.z1, train_data.z2, train_data.z3, target_fairness = 'dp', parameters = parameters, replacement = False, seed = seed)\n","    train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n","\n","    # ---------------------\n","    #  Model training\n","    # ---------------------\n","    for epoch in range(450):\n","        print(epoch, end=\"\\r\")\n","        \n","        tmp_loss = []\n","        \n","        for batch_idx, (data, target, z1, z2, z3) in enumerate (train_loader):\n","            loss = run_epoch (model, data, target, optimizer, criterion)\n","            tmp_loss.append(loss)\n","            \n","        losses.append(sum(tmp_loss)/len(tmp_loss))\n","        \n","\n","    tmp_test_z1 = test_model(model, xz_test, y_test, z1_test)\n","    tmp_test_z2 = test_model(model, xz_test, y_test, z2_test)\n","    tmp_test_z3 = test_model(model, xz_test, y_test, z3_test)\n","    full_tests_z1.append(tmp_test_z1)\n","    full_tests_z2.append(tmp_test_z2)\n","    full_tests_z3.append(tmp_test_z3)\n","    \n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z1['Acc'], tmp_test_z1['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z2['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")\n","    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test_z2['Acc'], tmp_test_z3['DP_diff']))\n","    print(\"----------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"8PUGxMJBPDYt","executionInfo":{"status":"aborted","timestamp":1643937346660,"user_tz":-540,"elapsed":24,"user":{"displayName":"Kim Vũ Trọng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08445343009736086420"}}},"outputs":[],"source":["\n","tmp_acc = []\n","tmp_dp_z1 = []\n","tmp_eo_z1 = []\n","tmp_dp_z2 = []\n","tmp_eo_z2 = []\n","tmp_dp_z3 = []\n","tmp_eo_z3 = []\n","for i in range(len(seeds)):\n","    tmp_acc.append(full_tests_z1[i]['Acc'])\n","print(\"Test accuracy (avg): {}\".format(sum(tmp_acc)/len(tmp_acc)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z1.append(full_tests_z1[i]['EqOdds_diff'])\n","    tmp_dp_z1.append(full_tests_z1[i]['DP_diff'])\n","print(\"z1 EO disparity  (avg): {}\".format(sum(tmp_eo_z1)/len(tmp_eo_z1)))\n","print(\"z1 DP disparity  (avg): {}\".format(sum(tmp_dp_z1)/len(tmp_dp_z1)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z2.append(full_tests_z2[i]['EqOdds_diff'])\n","    tmp_dp_z2.append(full_tests_z2[i]['DP_diff'])\n","print(\"z2 EO disparity  (avg): {}\".format(sum(tmp_eo_z2)/len(tmp_eo_z2)))\n","print(\"z2 DP disparity  (avg): {}\".format(sum(tmp_dp_z2)/len(tmp_dp_z2)))\n","\n","for i in range(len(seeds)):\n","    tmp_eo_z3.append(full_tests_z3[i]['EqOdds_diff'])\n","    tmp_dp_z3.append(full_tests_z3[i]['DP_diff'])\n","print(\"z3 EO disparity  (avg): {}\".format(sum(tmp_eo_z3)/len(tmp_eo_z3)))\n","print(\"z3 DP disparity  (avg): {}\".format(sum(tmp_dp_z3)/len(tmp_dp_z3)))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"FairRobust_synthetic_grouptargeted_label_flipping.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}